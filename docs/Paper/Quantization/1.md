# 
#### Paper: [A Comprehensive Survey on Model Quantization for Deep Neural Networks in Image Classification](https://dl.acm.org/doi/10.1145/3623402){target="_blank"}
!!! abstract
    - 背景：Deep Neural Networks (DNNs) 在机器学习领域取得了重大进步，优点是精度高，缺点是存储消耗高，耗能高，这使得他们在有限的硬件资源上难以使用，量化 （Quantization）（the full-precision values are stored in low bit-width precision）即为解决这一难题的方法之一。
    - Quantization的优点：1. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. 2. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods.
    - 贡献：
        1. Consequently, we present a comprehensive **survey of quantization** concepts and methods, with a focus on image classification. 
        2. We describe **clustering-based quantization** methods and explore **the use of a scale factor** parameter for approximating full-precision values. 
        3. Moreover, we thoroughly review **the training of a quantized DNN**, including the use of a straight-through estimator and quantization regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. 
        4. Furthermore, we highlight the evaluation **metrics for quantization methods** and important **benchmarks in the image classification task**. We also present the accuracy of the state-of-the-art methods on CIFAR-10 and ImageNet. 
        5. This article attempts to make the readers familiar with the basic and advanced concepts of quantization, introduce important works in DNN quantization, and highlight challenges for future research in this field.
## 1 简介
1. Deep Convolutional Neural Network (DCNN) 成就斐然，但需要存储大量参数，进行大量计算（The main operation in DCNNs is multiply-accumulate
(MAC) in convolution and Fully Connected (FC) layers.），所以DNNs的加速很有必要。
2. In the beginning, the focus was on hardware optimization for processing speedup in DNN accelerators.  
->  Later, researchers concluded that compression
and software optimization of DNNs can be more effective before touching hardware.
3. The approaches in DNN compression:
    - **Quantization**: approximates the numerical network components with low bit-width precision.
    - **Pruning**: removing unnecessary or less important connections within the network and
    making a sparse network that reduces memory usage as well as computations.
    - **Low-rank approximation**: an approach to simplify matrices and images, creates a
    new matrix close to the weight matrix, which has lower dimensions and fewer computations in DNNs.
    - **Knowledge Distillation (KD)**: employ a simpler model that exhibits generalization and accuracy comparable to the complex model.
    !!! note "Advantages of quantization"
        - High compression, with less accuracy reduction.
        - Flexibility  
        -- Since quantization is not dependent on the network architecture, a quantization algorithm can be
        applied to various types of DNNs. (Many quantization methods originally designed for
        DCNNs are also used for Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks) .
        - Smaller number of cycles on hardware  
        -- as high-cost floating-point operations are replaced with low-cost operations.
        - Reduces the cost of hardware accelerator design.  
        -- For instance, in 1-bit quantization, a 32-bit floating-point multiplier can be replaced an XNOR operator, leading to a
        cost reduction of 200 times in Xilinx FPGA
        - Contribute to controlling
        overfitting.  
        -- By simplifing parameters
4. 列举了一些关于各种Quantization方式的参考文献 (binary,mixed-precision,distillation-assisted,hardware-aware......)
5. 概况后续内容 (同abstract)
## 2 神经网络基本概念
1. A DCNN consists of various types of layers, and the common layers include **convolution layer, normalization layer, pooling layer, and FC layer**.
2. - The main layer in DCNN is the convolution layer, which is formed in three
    dimensions. 
    - This layer produces an output feature map by convolving multiple filters (weights)
    with the input feature map. 
    - There is weight sharing in the convolution layer, which means that
    each weight is applied to different connections. 
    - The majority of computationsin DCNNs are in this
    layer due to its three-dimensional structure and **weight sharing**.
3. **Weight sharing** -> a significant reduction in the number of parameters.  
-> the majority of parameters in DCNNs are typically in the FC layers, where each neuron
is connected to all neurons in both the previous and next layers.
4. As **the convolution and FC layers**
contain the majority of **computations and parameters in DCNNs**, the primary focus is on these
layers in accelerators and compression techniques.
## 3 量化的概念
Quantization is mapping values from a continuous space to a discrete space, where full-precision
values are mapped to new values with lower bit-width called quantization levels.
### 3.1 (网络组成中)量化的对象
- Each numerical component in neural networks can be quantized. ‌These components are typically divided into three main categories: weights, activations, and gradients.
- Weights: the most common (But in
most cases, biases and other parameters, such as batch normalization parameters, are kept in full
precision in view of the fact that they include a minimal rate of neural network parameters, and the
quantization of them is less efficient in compression.)
- Activations: 比weights的量化更困难 (While
weightsremain fixed after training, activations change during the inference phase according to the
input data.)，但仅仅对weights量化效率不高、内存使用率低, 故需共同量化
- Gradients: 仅对训练时的加速有用，更难量化 (While
weightsremain fixed after training, activations change during the inference phase according to the
input data.)
### 3.2 何时量化
| | Quantization-Aware Training (QAT)|Post-Training Quantization (PTQ).|
|---|---|---|
|When| After training| During training|
|Defect| In a low bit-width precision quantized network, the convergence of the learning algorithm is challenging. -> requires more iterations than the full-precision network for convergence. It needs customized solutions compatible with a discrete network.| A reduction in model accuracy. -> needs retraining, fine-tuning agter quantization -> repeat to reach an acceptable accuracy.|
|Speedup phase|the training and inference phases|the inference phase|

- The model accuracy in the QAT approach is commonly higher than in PTQ, because the trained
model is more compatible with the quantization process.
### 3.3 确定性(deterministic)和随机(stochastic)量化
#### Deterministitc Quantization Methods
- Binary quantization:

$$b=\text{Sign}\left(x\right)=\begin{cases}+1\quad x\geq0\\-1\quad x<0\end{cases}.$$

- Ternary quantization:

$$t=\begin{cases}+1&x>\Delta\\0&|x|\leq\Delta\\-1&x<-\Delta\end{cases}.$$

- Others:

$$Q\left(x\right)=Sign\left(x\right).d.\min\left(round\left(\frac{\left|x\right|}d\right),\frac{M-1}2\right)$$

d represents the step size, and M is an odd number and determines the number
of quantization levels. Consequently, the quantization levels include zero, positive, and negative
values symmetrically.

$$quantize_k=\frac1{2^k-1}round\left(\left(2^k-1\right)x\right),0\leq x\leq1$$

It maps the full-precision values in the range $x\in[0,1]$ to $2^k$ quantization levels within the same interval with step size $\frac1{2^k-1}.$ For $k$ bit-width, the quantization levels are $L_q=$ $\{0,\frac1{2^k-1},\frac2{2^k-1},\ldots,1\}.$ For example, for $k=2$,there are $2^2=4$ quantization levels, which are $L_q=$ $\{ 0$, 1/3, 2/3, $1\} .$

- Learned Quantization Network (LQ-Nets):

$$Q\left(x,v\right)=v^Te_l\ e_l\in\left\{-1,1\right\}^K,x\in\left(t_l,t_{l+1}\right).$$

x represents full-precision values, $\nu\in\mathbb{R}^{K}$ denotes the learnable floating-point basis vector, and $e_{l}$ is a $k$-bit binary vector from $[-1,-1,\ldots,-1]\mathrm{~to~}[1,1,\ldots,1].$
#### Stochastic Quantization Methods

$$b=\begin{cases}+1\:p=\sigma\:(x)\\-1\:q=1-p\end{cases}.$$

$$\sigma\left(x\right)=clip\left(\frac{x+1}2,0,1\right)=\max\left(0,\min\left(1,\frac{x+1}2\right)\right).$$

$$\begin{cases}\mathrm{if~}w>0:p\left(t=1\right)=w;\quad p\left(t=0\right)=1-w\\\mathrm{if~}w<0:p\left(t=-1\right)=-w;p\left(t=0\right)=1+w\end{cases}.$$

#### Deterministic and Stochastic Quantization Comparison.
- Stochastic quantization has
shown **better model generalization** compared to deterministic quantization.
- Implementation of stochastic quantization is **more challenging and costly** than deterministic quantization, particularly in hardware implementations, as it
requires a random bit generator.