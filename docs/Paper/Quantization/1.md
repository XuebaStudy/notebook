# 
#### Paper: [A Comprehensive Survey on Model Quantization for Deep Neural Networks in Image Classification](https://dl.acm.org/doi/10.1145/3623402)
!!! abstract
    - 背景：Deep Neural Networks (DNNs) 在机器学习领域取得了重大进步，优点是精度高，缺点是存储消耗高，耗能高，这使得他们在有限的硬件资源上难以使用，量化 （Quantization）（the full-precision values are stored in low bit-width precision）即为解决这一难题的方法之一。
    - Quantization的优点：1. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. 2. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods.
    - 贡献：
        1. Consequently, we present a comprehensive **survey of quantization** concepts and methods, with a focus on image classification. 
        2. We describe **clustering-based quantization** methods and explore **the use of a scale factor** parameter for approximating full-precision values. 
        3. Moreover, we thoroughly review **the training of a quantized DNN**, including the use of a straight-through estimator and quantization regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. 
        4. Furthermore, we highlight the evaluation **metrics for quantization methods** and important **benchmarks in the image classification task**. We also present the accuracy of the state-of-the-art methods on CIFAR-10 and ImageNet. 
        5. This article attempts to make the readers familiar with the basic and advanced concepts of quantization, introduce important works in DNN quantization, and highlight challenges for future research in this field.
## 1 简介
1. Deep Convolutional Neural Network (DCNN) 成就斐然，但需要存储大量参数，进行大量计算（The main operation in DCNNs is multiply-accumulate
(MAC) in convolution and Fully Connected (FC) layers.），所以DNNs的加速很有必要。
2. In the beginning, the focus was on hardware optimization for processing speedup in DNN accelerators.  
->  Later, researchers concluded that compression
and software optimization of DNNs can be more effective before touching hardware.
3. The approaches in DNN compression:
    - **Quantization**: approximates the numerical network components with low bit-width precision.
    - **Pruning**: removing unnecessary or less important connections within the network and
    making a sparse network that reduces memory usage as well as computations.
    - **Low-rank approximation**: an approach to simplify matrices and images, creates a
    new matrix close to the weight matrix, which has lower dimensions and fewer computations in DNNs.
    - **Knowledge Distillation (KD)**: employ a simpler model that exhibits generalization and accuracy comparable to the complex model.
    !!! note "Advantages of quantization"
        - High compression, with less accuracy reduction.
        - Flexibility  
        -- Since quantization is not dependent on the network architecture, a quantization algorithm can be
        applied to various types of DNNs. (Many quantization methods originally designed for
        DCNNs are also used for Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks) .
        - Smaller number of cycles on hardware  
        -- as high-cost floating-point operations are replaced with low-cost operations.
        - Reduces the cost of hardware accelerator design.  
        -- For instance, in 1-bit quantization, a 32-bit floating-point multiplier can be replaced an XNOR operator, leading to a
        cost reduction of 200 times in Xilinx FPGA
        - Contribute to controlling
        overfitting.  
        -- By simplifing parameters
4. 列举了一些关于各种Quantization方式的参考文献 (binary,mixed-precision,distillation-assisted,hardware-aware......)
5. 概况后续内容 (同abstract)
## 2 神经网络基本概念
1. A DCNN consists of various types of layers, and the common layers include **convolution layer, normalization layer, pooling layer, and FC layer**.
2. - The main layer in DCNN is the convolution layer, which is formed in three
    dimensions. 
    - This layer produces an output feature map by convolving multiple filters (weights)
    with the input feature map. 
    - There is weight sharing in the convolution layer, which means that
    each weight is applied to different connections. 
    - The majority of computationsin DCNNs are in this
    layer due to its three-dimensional structure and **weight sharing**.
3. **Weight sharing** -> a significant reduction in the number of parameters.  
-> the majority of parameters in DCNNs are typically in the FC layers, where each neuron
is connected to all neurons in both the previous and next layers.
4. As **the convolution and FC layers**
contain the majority of **computations and parameters in DCNNs**, the primary focus is on these
layers in accelerators and compression techniques.
## 3 量化的概念
