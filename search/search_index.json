{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-xuebas-notebook","title":"Welcome to xueba's notebook!","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Studing?};\n  B --&gt;|Yes| C[Good!];\n  C --&gt;|Energetic| B;\n  C --&gt;|Tired| D[Ask yourself];\n  D --&gt;|By changing task| B;\n  B --&gt;|No| D;\n  D --&gt;|Need break| E[Take a break];\n  E --&gt;|OK| A;\n  A -----&gt;F[Other things];</code></pre>"},{"location":"Belief/sentences/","title":"sentences","text":"<p>Quote</p> <p>Quote</p> <p>Quote</p> <p>Quote</p> <p>Quote</p> <p>Quote</p> <p>\u591a\u6101\u5584\u611f\u3001\u80e1\u601d\u4e71\u60f3\u3001\u7126\u8651\u81ea\u6ee1\uff0c\u90fd\u662f\u201c\u95f2\u201d\u51fa\u6765\u7684\uff0c\u4e0d\u662f\u201c\u5fd9\u201d\u51fa\u6765\u7684\u3002\u6709\u70b9\u4e8b\u505a\uff0c\u5c31\u597d\u4e86\u3002</p> <p>Quote</p>"},{"location":"Belief/sentences/#_1","title":"\u667a\u8005\u4e0d\u4f1a\u628a\u81ea\u5df1\u9677\u4e8e\u5371\u9669\u7684\u5883\u5730","text":""},{"location":"Belief/sentences/#_2","title":"\u8d1f\u71b5","text":""},{"location":"Belief/sentences/#_3","title":"\uff08\u751f\u6d3b\u3001\u5b66\u4e60\u5b89\u6392\u4e0a\u7684\uff09\u8d2a\u5fc3\u7b97\u6cd5","text":""},{"location":"Belief/sentences/#_4","title":"\u7528\u575a\u6301\u8868\u8fbe\u5bf9\u4fe1\u4ef0\u7684\u7b03\u5b9a","text":""},{"location":"Belief/sentences/#_5","title":"\u9634\u9633\u8f6c\u6362","text":""},{"location":"Belief/sentences/#_6","title":"\u7f04\u9ed8","text":""},{"location":"Belief/charactors/%E6%B4%9B%E7%90%AA%E5%B8%8C/","title":"\u6d1b\u742a\u5e0c","text":""},{"location":"Belief/charactors/%E6%B4%9B%E7%90%AA%E5%B8%8C/#_1","title":"\u6d1b\u742a\u5e0c","text":""},{"location":"Belief/charactors/%E7%81%B0%E4%B9%8B%E9%AD%94%E5%A5%B3%E4%BC%8A%E8%95%BE%E5%A8%9C/","title":"\u7070\u4e4b\u9b54\u5973\u4f0a\u857e\u5a1c","text":""},{"location":"Belief/charactors/%E7%81%B0%E4%B9%8B%E9%AD%94%E5%A5%B3%E4%BC%8A%E8%95%BE%E5%A8%9C/#_1","title":"\u7070\u4e4b\u9b54\u5973\u4f0a\u857e\u5a1c","text":""},{"location":"Diary/Reflection/Others/","title":"Others","text":"<p>Abstract</p> <p>\u201cOthers\u201d\u4e2d\u8bb0\u5f55\u7684\u53cd\u601d\u5e76\u4e0d\u662f\u8bf4\u4e0d\u91cd\u8981\uff0c\u53ea\u662f\u53ef\u91cd\u590d\u6027\u76f8\u5bf9\u8f83\u5dee\u7f62\u4e86\uff08\u4e00\u751f\u80fd\u6709\u51e0\u6b21\u634f\uff09\u3002</p>"},{"location":"Diary/Reflection/Others/#20241124-cet-6-oral-test","title":"2024.11.24 CET-6 oral test","text":"A bit too much <ol> <li> <p>\u53ef\u4ee5\u62c9\u6ee1\u7684\u7b54\u9898\u65f6\u95f4\uff0c\u5c3d\u91cf\u5148\u6253\u6ee1\uff0c\u5426\u5219\u5373\u4f7f\u4f60\u63d0\u524d\u603b\u7ed3\u597d\u4e86\uff0c\u4f46\u662f\u5269\u4e0b\u6765\u7684\u90a3\u51e0\u5341\u79d2\u7a7a\u95f2\u7684\u65f6\u95f4\u4f1a\u8ba9\u4f60\u8d8a\u6765\u8d8a\u7d27\u5f20\uff0c\u8ba9\u4f60\u5f00\u59cb\u6000\u7591\u81ea\u5df1\uff0c\u8fd9\u53cd\u800c\u4e0d\u5229\u4e8e\u540e\u7eed\u7684\u7b54\u9898</p> </li> <li> <p>\u5728\u6b63\u5f0f\u8003\u8bd5\u4e4b\u524d\uff0c\u6709\u4e00\u5c0f\u6bb5\u548c\u4f60\u7684partner\u5bf9\u8bdd\u7684\u65f6\u95f4\uff0c\u4f60\u4eec\u4e00\u5b9a\u8981\u5728\u8fd9\u4e00\u6bb5\u65f6\u95f4\u91cc\u9762\u5148\u89c4\u5212\u597d\uff0c\u5230\u65f6\u5019\u7684\u4ea4\u6d41\u9636\u6bb5\u8c01\u6765\u5148\u53d1\u95ee\u8c01\u6765\u7b54\u4ec0\u4e48\u8c01\u6765\u505a\u603b\u7ed3\uff0c\u5426\u5219\u5728\u90a3\u4e2a\u9636\u6bb5\u4e2d\u518d\u6765\u4ea4\u6d41\u7684\u8bdd\u5c31\u4f1a\u663e\u5f97\u624b\u5fd9\u811a\u4e71</p> </li> <li> <p>\u7f51\u4e0a\u6240\u8bf4\u7684\u5404\u79cd\u53e3\u8bed\u8003\u8bd5\u5e38\u89c1\u4e3b\u9898\u662f\u771f\u7684\u662f\u5e38\u89c1\u7684\uff0c\u771f\u7684\u5f88\u5bb9\u6613\u8003\u5f97\u5230\uff0c\u6bd4\u5982\u6211\u8fd9\u4e00\u6b21\u5c31\u8003\u4e86\u8003\u70c2\u4e86\u7684\u65c5\u6e38\u3002\u6240\u4ee5\u6709\u673a\u4f1a\u4e00\u5b9a\u8981\u63d0\u524d\u51c6\u5907\uff0c\u90a3\u4e48\u5341\u51e0\u4e2a\u4e3b\u9898\uff0c\u4ed6\u4eec\u7684\u8bdd\u9898\u8bcd\u6c47\u4ee5\u53ca\u5404\u79cd\u89c2\u70b9\u3002\u4e0d\u8981\u89c9\u5f97\u7528\u4e0d\u4e0a</p> </li> <li> <p>\u5e73\u65f6\u8bad\u7ec3\u7684\u65f6\u5019\u5e94\u8be5\u591a\u8bad\u7ec3\u81ea\u5df1\uff0c\u8fb9\u8bf4\u8bdd\u8fb9\u60f3\u4e0b\u4e00\u53e5\u8bdd\u7684\u80fd\u529b\uff0c\u771f\u6b63\u8003\u8bd5\u7684\u65f6\u5019\u5728\u90a3\u4e9b\u7b80\u77ed\u7684\u65f6\u95f4\u91cc\u9762\u4e0d\u4f1a\u7ed9\u4f60\uff0c\u6709\u65f6\u95f4\u63d0\u524d\u5148\u5168\u90e8\u60f3\u597d\u518d\u6765\u8bf4\u7684</p> </li> <li> <p>\u63d0\u524d\u51c6\u5907\u597d\u4e00\u4e9b\u5e38\u7528\u53e5\u5b50\uff0c\u7528\u6765\u8854\u63a5\u81ea\u5df1\u7d27\u5f20\u5230\u5f00\u59cb\u8bf4\u8f66\u8f71\u8f98\u8bdd\uff0c\u6216\u8005\u662f\u8bf4\u4e0d\u51fa\u8bdd\u7684\u65f6\u5019.\u8fd9\u4e9b\u53e5\u5b50\u53ef\u80fd\u4fd7\u70c2\u5230\u5e76\u4e0d\u53ef\u4ee5\u6210\u4e3a\u52a0\u5206\u70b9\uff0c\u4f46\u662f\u5b83\u5374\u53ef\u4ee5\u5f88\u597d\u7684\u7f13\u89e3\u4f60\u7684\u60c5\u7eea\uff0c\u5e2e\u52a9\u4f60\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u56de\u7b54</p> </li> <li> <p>\u9664\u975e\u4f60\u771f\u7684\u6709\u6781\u597d\u7684\u53e3\u8bed\u80fd\u529b\uff0c\u5426\u5219\u5404\u79cd\u5404\u6837\u56de\u7b54\uff0c\u5404\u79cd\u5404\u6837\u7c7b\u578b\u95ee\u9898\u7684\u7b54\u9898\u6a21\u677f\u5957\u8def\u8bf7\u80cc\u719f\uff0c\u56e0\u4e3a\u80cc\u719f\u4e4b\u540e\u771f\u7684\u6709\u673a\u4f1a\u6551\u4f60\u4e00\u547d\uff0c\u5373\u4f7f\u4f60\u5f80\u91cc\u9762\u586b\u5145\u7684\u4e1c\u897f\u90fd\u662f\u4e00\u5806\u5c4e\uff0c\u4f46\u662f\u81f3\u5c11\u8ba9\u522b\u4eba\u77e5\u9053\u4f60\u662f\u8bf4\u4e86\u4e1c\u897f\u7684</p> </li> <li> <p>\u6700\u540e\u4e00\u6761\uff0c\u4e0d\u8981\u5fd8\u8bb0\u5e26\u51c6\u8003\u8bc1\u4ee5\u53ca\u8eab\u4efd\u8bc1</p> </li> </ol>"},{"location":"Diary/Reflection/exams/","title":"exams","text":"Abstract <ul> <li>\u8fd9\u91cc\u8bb0\u5f55\u4e00\u4e9b\u5173\u4e8e\u8003\u8bd5\u7684\u53cd\u601d\u3002\u5f53\u7136\uff0c\u4f5c\u4e3a\u5b66\u751f\uff0c\u8003\u8bd5\u4e0d\u53ef\u80dc\u6570\uff0c\u5e7f\u4e49\u7684\u201c\u8003\u8bd5\u201d\u66f4\u662f\u4e0e\u6211\u4eec\u7684\u751f\u6d3b\u5f62\u5f71\u76f8\u968f\uff0c\u6240\u4ee5\u4e5f\u4e0d\u53ef\u80fd\u5168\u90e8\u8bb0\u5f55\u4e0b\u6765\u3002  </li> <li>\u4f46\u662f\u663e\u800c\u6613\u89c1\u7684\u662f\uff0c\u8003\u8bd5\uff0c\u603b\u662f\u626e\u6f14\u7740\u950b\u5229\u7684\u624b\u672f\u5200\uff0c\u5b83\u80fd\u591f\u5145\u5206\u3001\u6beb\u4e0d\u7559\u60c5\u5730\u66b4\u9732\u6211\u4eec\u7684\u95ee\u9898\uff08\u65e0\u8bba\u662f\u4e8b\u524d\u51c6\u5907\uff0c\u8fd8\u662f\u4e34\u573a\u53d1\u6325\u7b49\uff09\u3002\u4e00\u573a\u610f\u4e49\u3001\u5f71\u54cd\u91cd\u5927\u7684\u8003\u8bd5\u7684\u5931\u8d25\uff0c\u80fd\u591f\u50cf\u4e00\u628a\u575a\u786c\u7684\u94c1\u9524\uff0c\u5728\u6bcf\u4e00\u6b21 accidentally remember it \u6216 directly been influenced by its result \u65f6,\u51fb\u788e\u6211\u4eec\u5185\u5fc3\u81ea\u6211\u611f\u89c9\u826f\u597d\u7684\u51b0\u5c42\uff0c\u8feb\u4f7f\u6211\u4eec\u5728\u75db\u82e6\u3001\u61ca\u6094\u3001\u9057\u61be\u3001\u4e0d\u7518\u3001\u7126\u8651...\u4e2d\uff0c\u6df1\u523b\u53cd\u601d\uff0c\u60f3\u51fa\u89e3\u51b3\u65b9\u6cd5\uff0c\u5e76\u4ed8\u8bf8\u5b9e\u65bd\u3002  </li> <li>\u7136\u800c\uff0c\u8981\u662f\u4e8b\u60c5\u771f\u5c31\u5982\u6b64\u7b80\u5355\uff0c\u4e5f\u5c31\u4e0d\u4f1a\u6709\u90a3\u4e48\u591a\u7ea0\u7ed3\u4e86\u3002<ol> <li>\u4e0d\u662f\u4eba\u4eba\u90fd\u80fd\u9f13\u8d77\u52c7\u6c14\u76f4\u9762\u81ea\u5df1\u7684\u5931\u8d25\uff0c\u626c\u957f\u907f\u77ed\u3001\u8d8b\u5229\u907f\u5bb3\u624d\u662f\u6211\u4eec\u7684\u672c\u80fd\u3002  </li> <li>\u5373\u4f7f\u613f\u610f\u8fdd\u9006\u81ea\u5df1\u8d2a\u56fe\u5b89\u9038\u7684\u672c\u80fd\uff0c\u4e5f\u5f88\u53ef\u80fd\u53d7\u9650\u4e8e\u773c\u754c\u3001\u6d88\u606f\u3001\u7ecf\u9a8c\u7b49\uff0c\u65e0\u6cd5\u603b\u7ed3\u51fa\u6700\u6709\u6548\u7684\u5e94\u5bf9\u65b9\u6848\u3002  </li> <li>\u201c\u77e5\u6613\u884c\u96be\u201d\uff0c\u5373\u4f7f\u6709\u4e86\u76f8\u5bf9\u5408\u7406\u7684\u8ba1\u5212\uff0c\u82e5\u4e0d\u80fd\u5728\u540e\u7eed\u751f\u6d3b\u4e2d\u5207\u5b9e\u5c65\u884c\uff0c\u4ee5\u4e0a\u52aa\u529b\u81ea\u7136\u7686\u4e3a\u5f92\u52b3\u3002  </li> <li>\u5f53\u7136\uff0c\u5728\u5931\u8d25\u7684\u5de8\u5927\u9a71\u52a8\u529b\u4e0b\uff0c\u521d\u671f\u7684\u52aa\u529b\u8fd8\u662f\u6bd4\u8f83\u5e38\u89c1\u7684\u3002\u4f46\u8fdb\u884c\u4e2d\uff0c\u4f60\u4f1a\u53d1\u73b0\uff0c\u4e5f\u8bb8\u4e00\u5f00\u59cb\u7684\u65b9\u6848\u8003\u8651\u4e0d\u5468\uff0c\u53ef\u80fd\u9700\u8981\u4f9d\u7167\u53d1\u5c55\u60c5\u51b5\u6539\u53d8\u3002\u4f46\u4e00\u65e6\u5f00\u59cb\u6539\u53d8\uff0c\u60f3\u8981\u589e\u52a0\u96be\u5ea6\uff0c\u65e0\u5f02\u4e8e\u9700\u8981\u518d\u6b21\u8d70\u51fa\u81ea\u5df1\u7684\u5b89\u9038\u533a\uff0c\u8c08\u4f55\u5bb9\u6613\uff1f\u82e5\u60f3\u8981\u51cf\u8f7b\u538b\u529b\uff0c\u53c8\u5f80\u5f80\u6700\u7ec8\u9677\u5165\u5e95\u7ebf\u4e0d\u65ad\u5d29\u584c\u7684\u6ce5\u6f6d\uff0c\u8fd9\u65f6\uff0c\u53ea\u9700\u8981\u4e00\u4e1d\u4e1d\u201c\u610f\u5916\u201d\uff08\u5982\u4e00\u6b21\u8fc7\u5206\u7684\u653e\u7eb5\u3001\u4e00\u4ef6\u770b\u4f3c\u66f4\u91cd\u8981\u7684\u4e8b\u7684\u51fa\u73b0\u3001\u8eab\u5fc3\u72b6\u6001\u7684\u5931\u8c03\uff09......\u7ed3\u679c\u81ea\u7136\u4e0d\u5fc5\u591a\u8a00\u3002  </li> <li>\u8fd9\u4e2a\u4e16\u754c\u662f\u5982\u6b64\u590d\u6742\uff0c\u4ee5\u81f3\u4e8e\u5f88\u591a\u4e8b\u7684\u597d\u574f\u5224\u65ad\u603b\u662f\u4f1a\u4e0d\u65ad\u53d8\u5316\u7740\uff0c\u5373\u4f7f\u4f60\u771f\u5b8c\u6210\u4e86\u65e2\u5b9a\u4efb\u52a1\uff0c\u53c8\u5982\u4f55\u5462\uff1f  </li> </ol> </li> <li>\u5176\u5b9e\u4e0a\u8ff0\u60c5\u51b5\u8fd8\u7b97\u597d\u7684\uff0c\u66f4\u52a0\u5e38\u89c1\u7684\u662f\uff1a\u521a\u5931\u8d25\u65f6\u8981\u4e48\u6028\u5929\u5c24\u4eba\uff0c\u8981\u4e48\u75db\u54ed\u6d41\u6d95\u3001\u53d1\u8a93\u8981\u600e\u6837\u600e\u6837\uff0c\u5e0c\u671b\u81ea\u5df1\u901a\u8fc7\u52aa\u529b\u6253\u4e00\u4e2a\u5927\u7ffb\u76d8\u3002\u53ef\u60dc\u554a\uff0c\u5f53\u65f6\u65e0\u8bba\u591a\u4e48\u81ea\u6211\u611f\u89c9\u75db\u5f7b\u5fc3\u6249\u7684\u611f\u60c5\u3001\u8a93\u8a00\uff0c\u5728\u201c\u65f6\u95f4\u201d\u8fd9\u5242\u826f\u836f\u9762\u524d\u662f\u591a\u4e48\u6e3a\u5c0f\u3002\u4e00\u6bb5\u65f6\u95f4\u8fc7\u540e\uff0c\u4e00\u5207\u90fd\u6de1\u5316\u3001\u6d88\u901d\u4e86\u3002  </li> <li>\u5f53\u7136\uff0c\u8fd9\u8fd8\u662f\u8981\u5f52\u529f\u4e8e\u201c\u4f60\u89c9\u5f97\u4f60\u73b0\u5728\u9047\u5230\u7684\u574e\u3001\u75db\u82e6\u8fc7\u4e0d\u53bb\u4e86\uff0c\u53ea\u662f\u56e0\u4e3a\u4f60\u6ca1\u9047\u5230\u8fc7\u66f4\u5927\u7684\u3002\u201d\u8fd9\u4e00\u4eba\u751f\u89c4\u5f8b\u3002\u5f53\u65f6\u95f4\u7ebf\u653e\u8fdc\u5230\u6574\u4e2a\u5b66\u671f\uff0c\u4e00\u6b21\u968f\u5802\u6d4b\u8bd5\u7684\u5f71\u54cd\u5c31\u4e0d\u5927\u4e86\uff1b\u5f53\u65f6\u95f4\u7ebf\u653e\u8fdc\u5230\u51e0\u5e74\uff0c\u4e00\u6b21\u671f\u672b\u8003\u8bd5\u7684\u5f71\u54cd\u5c31\u4e0d\u5927\u4e86\uff1b\u5f53\u653e\u773c\u6574\u4e2a\u4eba\u751f\uff0c\u751a\u81f3\u5f71\u54cd\u4eba\u751f\u8d70\u5411\u7684\u201c\u5927\u8003\u201d\u90fd\u53ef\u4ee5\u4e3e\u51fa\u5341\u51e0\u4e2a\u4e86\uff0c\u96be\u5206\u9ad8\u4e0b\u3002  </li> <li>\u5f53\u7136\uff0c\u6211\u4e0d\u662f\u8bf4\u6211\u4eec\u5c31\u4e0d\u8ba4\u771f\u5bf9\u5f85\u6bcf\u4e00\u6b21\u8003\u8bd5\u4e86\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6b63\u662f\u8fd9\u6570\u4e0d\u80dc\u6570\u7684\u5c0f\u3001\u5927\u8003\u8bd5\u4e00\u8d77\u9020\u5c31\u4e86\u72ec\u7279\u7684\u4f60\u3002\u65e0\u8bba\u662f\u5426\u8ba4\u771f\u51c6\u5907\uff0c\u662f\u5426\u8ba4\u771f\u53cd\u601d\uff0c\u4e00\u8d77\u53d6\u51b3\u4e8e\u4f60\u81ea\u5df1\uff0c\u4f60\u53ef\u4ee5\u81ea\u7531\u5730\u9009\u62e9\uff0c\u5e76\u83b7\u53d6\u76f8\u5e94\u56de\u62a5\uff0c\u627f\u62c5\u76f8\u5e94\u540e\u679c\u3002  </li> <li>\u6240\u4ee5\u5230\u5e95\u8981\u600e\u4e48\u529e\u5462\uff1f......\uff08\u60f3\u4e86\u8bb8\u591a\uff0c\u601d\u8fa9\u4e86\u8bb8\u591a\uff09  </li> <li>\u54ce\u54ce\u54ce\uff0c\u6211\u5728\u5e72\u4ec0\u4e48\u5440\uff1f\u6211\u5728\u7cbe\u795e\u5185\u8017\u5417\uff1f\u5b8c\u4e86\uff0cBelief \u4e2d \u201c\u591a\u6101\u5584\u611f\u3001\u80e1\u601d\u4e71\u60f3\u3001\u7126\u8651\u81ea\u6ee1\uff0c\u90fd\u662f\u2018\u95f2\u2019\u51fa\u6765\u7684\uff0c\u4e0d\u662f\u2018\u5fd9\u2019\u51fa\u6765\u7684\u3002\u6709\u70b9\u4e8b\u505a\uff0c\u5c31\u597d\u4e86\u3002\u201d \u8fd9\u53e5\u8bdd\u7684\u542b\u91d1\u91cf\u8fd8\u5728\u63d0\u9ad8\uff01  </li> <li>\u5728\u5e7d\u6697\u5b64\u72ec\u7684\u5c81\u6708\u4e2d\uff0c\u6709\u4fe1\u4ef0\u966a\u4f34\u7740\u6211\uff0c\u6211\u600e\u4e48\u8fd8\u80fd\u81ea\u79c1\u5230\u4e0d\u52aa\u529b\u56de\u62a5\uff0c\u53cd\u800c\u5728\u8fd9\u91cc\u5185\u8017\uff1f  </li> <li>\u90a3\u5c31\u8d76\u7d27\u603b\u7ed3\u4e00\u4e0b\u5427\uff0c\u5927\u6982\u8fd8\u662f\u201c\u751f\u547d\u5728\u4e8e\u8d1f\u71b5\u201d\uff0c\u5176\u4f59\u7684\u6211\u5df2\u7ecf\u4e0d\u60f3\u518d\u8c08\u8bba\u4e86\u3002</li> <li>\u4fe1\u4ef0\u554a\uff0c\u4f60\u662f\u5982\u6b64\u5bbd\u5bb9\u800c\u5584\u826f\uff0c\u6073\u8bf7\u4f60\u518d\u4e00\u6b21\u5bbd\u6055\u6211\u8ff7\u832b\u800c\u53c8\u6127\u759a\u7684\u5fc3\u7075\uff0c\u6211\u5fc5\u987b\u8981\u66f4\u52a0\u52aa\u529b\uff0c\u5426\u5219\u600e\u4e48\u5bf9\u5f97\u8d77\u4f60\u7684\u966a\u4f34\u4e0e\u671f\u5f85\u554a\uff01</li> </ul>"},{"location":"Diary/Reflection/exams/#20241117-iih-mid-exam","title":"2024.11.17 \u666e\u901a\u7269\u7406\u5b66II(H)  mid-exam","text":"<ul> <li>\u8003\u524d\u8fc7\u5ea6\u8bad\u7ec3\uff0c\u7528\u7b14\u6f14\u7b97\u5b9e\u64cd\uff08\u4e00\u76f4\u770b\u9898\u4f5c\u7528\u975e\u5e38\u7247\u9762\uff09\uff0c\u63d0\u524d\u7740\u624b\u590d\u4e60\uff08\u8003\u524d\u6025\u5306\u5306\u7684\u590d\u4e60\u6548\u7387\u867d\u9ad8\uff0c\u4f46\u603b\u5f52\u6ca1\u529e\u6cd5\u771f\u6b63\u5168\u9762\u201c\u8d2f\u901a\u201d\uff0c\u5fc3\u865a\u7684\u6837\u5b50\u4e0d\u7b26\u5408\u6211\u7684\u6218\u672f\u7f8e\u5b66\uff09\u3002  </li> <li>\u5982\u679c\u6709\u4e86\u65b0\u601d\u8def\u53ca\u7b54\u6848\uff0c\u6ca1\u628a\u63e1\u7684\u60c5\u51b5\u4e0b\u4e5f\u4e0d\u5fc5\u5212\u53bb\u539f\u6765\u90a3\u4e2a......</li> </ul>"},{"location":"Diary/Reflection/interview/","title":"interview","text":"<p>Abstract</p> <p>\u8fd9\u91cc\u8bb0\u5f55\u4e00\u4e9b\u53c2\u4e0e\u8fc7\u7684\u9762\u8bd5\u7684\u53cd\u601d\uff0c\u7559\u4e88\u81ea\u5df1\u4e0e\u8bfb\u8005\u4e3a\u6212\u3002\uff08\u4f46\u4e0d\u662f\u6240\u6709\u5751\u90fd\u51fa\u73b0\u8fc7\uff0c\u6709\u7684\u662f\u81ea\u5df1\u9644\u52a0\u7684\u601d\u8003\uff09</p>"},{"location":"Diary/Reflection/interview/#20241011","title":"2024.10.11 \u6d59\u6c5f\u5927\u5b66\u4e00\u7b49\u5956\u5b66\u91d1\u9762\u8bd5","text":"<p>\u5f62\u5f0f</p> <ul> <li>\u4e00\u7ec4\u4eba\u6570:6  \uff1b\u9762\u8bd5\u5b98\u4eba\u6570:3  \uff1b\u65f6\u957f:15-20min  \uff1b\u76ee\u7684:\u68c0\u9a8c\u8d44\u683c  </li> <li>\u6d41\u7a0b: \u6bcf\u4eba\u81ea\u6211\u4ecb\u7ecd1min -&gt; \u6bcf\u4eba\u88ab\u63d0\u4e0e\u6240\u63d0\u4ea4\u6750\u6599\u3001\u81ea\u6211\u4ecb\u7ecd\u6709\u5173\u76841~2\u4e2a\u95ee\u9898</li> </ul> <ul> <li>\u5e73\u65f6\u591a\u52aa\u529b\uff0c\u201c\u505a\u591a\u4e8b\u201d\u4e0d\u5982\u201c\u5c11\u4e8b\u4f18\u201d(\u9664\u975e\u4f60\u771f\u80fd\u91cf\u5316\u505a\u4e86\u591a\u5c11)</li> <li>\u63d0\u4ea4\u6750\u6599\u3001\u7533\u8bf7\u7406\u7531\u4e00\u5b9a\u4e00\u5b9a\u8981\u4e25\u8c28\u771f\u5b9e\uff0c\u4eba\u5bb6\u662f\u6839\u636e\u8fd9\u4e2a\u63d0\u95ee\u7684\uff0c\u8fd9\u4e0d\u662f\u83b7\u5956\u611f\u8a00</li> <li>\u81ea\u6211\u4ecb\u7ecd\u8981\u201c\u8fc7\u5ea6\u51c6\u5907\u201d\uff0c\u51c6\u5907200%\uff0c\u5b9e\u964580%\uff0c\u51c6\u5907100%\uff0c\u5b9e\u964540%</li> <li>\u81ea\u6211\u4ecb\u7ecd\u6311\u91cd\u70b9\uff0c\u4e0d\u8981\u7ed9\u81ea\u5df1\u7559\u4e0b\u6a21\u7cca\u7684\u201c\u8d28\u8be2\u4f59\u5730\u201d</li> <li>\u9047\u5230\u9762\u8bd5\u5b98\u95ee\u65e0\u5173\u75db\u75d2\u4e4b\u4e8b\uff08\u5982\u7533\u8bf7\u7406\u7531\u4e0d\u89c4\u8303\uff09\uff0c\u8bda\u6073\u9053\u6b49\uff0c\u8bf4\u660e\u4e0b\u6b21\u6539\u6b63</li> <li>\u65e2\u7136\u662f\u63d0\u4ea4\u8fc7\u7684\u3001\u4ecb\u7ecd\u8fc7\u7684\u81ea\u5df1\u505a\u8fc7\u7684\u4e8b\uff0c\u8bf7\u52a1\u5fc5\u4fdd\u8bc1\u80fd\u591f\u62d3\u5c55 \uff08\u6bd4\u5982\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u795e\u7ecf\u7f51\u7edc\uff0c\u4f60\u4e0d\u4f1a\u4e0d\u77e5\u9053\u524d\u51e0\u5929\u516c\u5e03\u7684\u8bfa\u8d1d\u5c14\u5316\u5b66\u5956\u662f\u7528\u4e86\u4eba\u5de5\u667a\u80fd\u9884\u6d4b\u86cb\u767d\u8d28\u7ed3\u6784\u5427\uff09\uff08\u8fd8\u597d\u6b63\u597d\u5173\u6ce8\u8fc7\uff0c\u8d8a\u52aa\u529b\uff0c\u8d8a\u5e78\u8fd0\uff09</li> </ul>"},{"location":"Diary/days_challenge/","title":"days challenge","text":""},{"location":"Diary/days_challenge/#days-challenge","title":"days challenge","text":"<p>Abstract</p> <p>\u4e00\u767e\u4e07\u5e74\u592a\u4e45\uff0c\u53ea\u4e89\u671d\u5915\uff01\u5168\u529b\u4ee5\u8d74\u5b8c\u6210\u6bcf\u5929\u7684\u6311\u6218\u5427\uff0c\u4fe1\u4ef0\u4e0e\u4f60\u540c\u5728\uff01</p> <p>\u5185\u5bb9:</p> <ul> <li>24.8: \u8bb0\u5f55\u00b7\u89c4\u8303\u6691\u5047\u6bcf\u5929\u7684\u884c\u4e3a</li> <li>24.10: \u8bb0\u5f55\u00b7\u89c4\u8303\u56fd\u5e86\u6bcf\u5929\u7684\u884c\u4e3a</li> <li>24.11: \u8bb0\u5f55\u00b7\u671f\u4e2d2\u5468 in ZJU</li> </ul>"},{"location":"Diary/days_challenge/24.10/","title":"24.10","text":""},{"location":"Diary/days_challenge/24.10/#2024101","title":"2024.10.1:","text":"<p>Things have done</p> <p>\u6982\u7387\u8bba/R2.1&amp;2.2/60min \u80cc\u5355\u8bcd/R70/25min \u8ba1\u7ec4/Lab1-report/65min \u79d1\u7814/RP/40min \u79d1\u7814/RP/35min pytorch\u5b66\u4e60/-2.1/45min \u80cc\u5355\u8bcd/R74/25min \u5348\u4f11/TL/210min \u6570\u503c\u5206\u6790/R2.2(LE)/50min \u79d1\u7814/RP/25min \u80cc\u5355\u8bcd/N60/20min \u5c0f\u63d0\u7434/Free/90min \u4f11\u95f2/bilibili/150min \u6570\u503c\u5206\u6790/R2.3-2.6/25min \u603b\u7ed3/maybe/15min</p> <p>Pie Chart:(1)</p> Introspection <p>\u4eca\u5929\u50cf\u662f\u505a\u4e86\u4e00\u4e9b\u4e8b\uff0c\u4f46\u611f\u89c9\u6548\u7387\u6709\u5f85\u52a0\u5f3a\uff0c\u8981\u66f4\u591a\u82b1\u65f6\u95f4\u5728\u5b66\u4e1a\u4e0a\uff0c\u51cf\u5c11\u201c\u65f6\u95f4\u8017\u6563\u201d\u3002\u5982\u679c\u6bcf\u5929\u90fd\u80fd\u8fbe\u5230\u671f\u672b\u5468\u7684\u6548\u7387\u5c31\u597d\u4e86</p>"},{"location":"Diary/days_challenge/24.10/#2024102","title":"2024.10.2:","text":"<p>Things have done</p> <p>\u6982\u7387\u8bba/N2.3&amp;H2.1/70min \u80cc\u5355\u8bcd/R80/25min \u6570\u503c\u5206\u6790/R6.n/25min \u4f11\u95f2/biliblili/80min \u6570\u503c\u5206\u6790/R6.n/60min \u6570\u503c\u5206\u6790/Lab3/40min \u5348\u4f11/NM/25min ADS/team-work/30min \u4e8b\u52a1/scholarship/30min \u8fd0\u52a8/NM/40min \u79d1\u7814/RP/135min AI\u5bfc/Lab2/100min</p> <p>Pie Chart:(1)</p> Introspection <p>......\u8fd8\u662f\u6548\u7387\u6ca1\u5230\u8fbe\u201c\u671f\u672b\u5468\u201d\u6c34\u5e73\uff0c\u800c\u4e14\u8fd9\u6837\u4e00\u56de\u987e\uff0c\u611f\u89c9\u4e00\u5929\u4e5f\u6ca1\u5e72\u4ec0\u4e48\uff0c\u4e5f\u8bb8\u8fd9\u5c31\u662f\u8bb0\u5f55\u65f6\u95f4\u7684\u9b45\u529b\u5427 \uff08\u5e2e\u52a9\u6211\u4eec\u8ba4\u8bc6\u5230\u81ea\u5df1\u4ec0\u4e48\u4e5f\u6ca1\u5e72\uff09 \u4e0d\u6562\u5962\u671b\u660e\u5929\uff0c\u4f46\u6211\u4f1a\u52aa\u529b\u7684\u3002\u4fe1\u4ef0\u6b63\u6ce8\u89c6\u7740\u6211\u5462\uff01\u6211\u4e0d\u80fd\u8f9c\u8d1f\uff01</p>"},{"location":"Diary/days_challenge/24.10/#2024103","title":"2024.10.3:","text":"<p>Things have done</p> <p>\u6982\u7387\u8bba/H2/50min \u80cc\u5355\u8bcd/R70/30min \u7269\u7406/R28/15min \u4f11\u606f/\u8865\u7720/130min \u7269\u7406/R28/55min \u80cc\u5355\u8bcd/R53/20min \u7269\u7406/R28/55min \u7269\u7406/H28/30min \u5348\u4f11/NM/20min \u6574\u7406/COnotes/20min \u8ba1\u7ec4/HW1/70min \u8fd0\u52a8/NM/30min \u80cc\u5355\u8bcd/N60/25min \u6570\u503c\u5206\u6790/Lab3/60min \u79d1\u7814/RP/90min pytorch/-2.2/30min pytorch/-2.3/40min \u603b\u7ed3/blog/20min</p> <p>Pie Chart:(1)</p> Introspection <p>\u65f6\u95f4\u5927\u6982\u5206\u4e3a\u6709\u6548\u65f6\u95f4\uff08\u542b\u9ad8\u6548\u65f6\u95f4\u4e0e\u4f4e\u6548\u65f6\u95f4\uff09\u3001\u65e0\u6548\u65f6\u95f4\uff08\u542b\u7410\u788e\u65f6\u95f4\u548c\u5783\u573e\u65f6\u95f4\uff09\u4e0e\u4f11\u606f\u65f6\u95f4\uff08\u542b\u5fc5\u8981\u4f11\u606f\u548c\u81ea\u7531\u4f11\u606f\uff09\u3002\u53ef\u4ee5\u4f11\u606f\uff0c\u5927\u5927\u65b9\u65b9\u5730\u8bb0\u5f55\u4e0b\u6765\uff08\u5982\u770b\u756a\uff09\uff0c\u8fd9\u5f88\u6b63\u5e38\u3002\u4f46\u8bf7\u52a1\u5fc5\u675c\u7edd\u5783\u573e\u65f6\u95f4\uff0c\u90a3\u4e9b\u6ca1\u8bb0\u5f55\u4e0b\u6765\u7684\u6574\u6bb5\u65f6\u95f4\u4e2d\u6709\u591a\u5c11\u5783\u573e\u65f6\u95f4\uff0c\u81ea\u5df1\u597d\u597d\u60f3\u4e00\u60f3\u5427\u3002\u5b9e\u8bdd\u8bf4\uff0c\u770b\u756a\u3001\u770b\u4fe1\u4ef0\uff0c\u90fd\u4f1a\u6bd4\u90a3\u4e9b\u4e8b\u66f4\u6709\u610f\u4e49</p>"},{"location":"Diary/days_challenge/24.11/","title":"24.11","text":""},{"location":"Diary/days_challenge/24.11/#2024114","title":"2024.11.4:","text":"<p>Things have done</p> <p>\u6982\u7387\u8bba/review/45min \u80cc\u5355\u8bcd/R100/25min \u666e\u7269/review/20min \u4f11\u606f/break/20min \u4e0a\u8bfe/physics/100min \u8fd0\u52a8/NM/40min ADS/HW/35min \u535a\u5ba2/Lessons/55min NA/review/25min NA/review/20min \u80cc\u5355\u8bcd/R100/15min \u80cc\u5355\u8bcd/R100/25min \u79d1\u7814/read paper/70min \u4f11\u606f/break/25min \u6570\u503c\u5206\u6790/HW/50min ADS/DP/20min \u8ba1\u7ec4/lab4.c/20min \u8ba1\u7ec4/review/30min</p> <p>Pie Chart:(1)</p> Introspection <p>\u6258\u4e00\u4f4d\u631a\u53cb\u7684\u798f\uff0c\u6211\u53c8\u5f00\u59cb\u8bb0\u5f55\u65f6\u95f4\u4e86\uff0c\u8fd9\u6b21\u662f\u65e5\u5e38\u5b66\u4e60\u751f\u6d3b\uff08\u4f46\u7531\u4e8e\u662f\u671f\u4e2d2\u5468\uff0c\u4f1a\u7d27\u5f20\u4e00\u4e9b\uff09\u3002\u767d\u5929\u6b63\u5e38\uff0c\u6548\u7387\u8fd8\u8981\u9ad8\u4e00\u4e9b\uff0c\u4f46\u665a\u4e0a\u53ef\u662f\u71ac\u5927\u591c\u4e86 (\u751a\u81f3\u4e0d\u662f\u5e72\u6b63\u4e8b) \u3002\uff08\u597d\u597d\u53cd\u601d\u4e00\u4e0b\uff0c\u81ea\u5df1\u5bf9\u5f97\u8d77\u4fe1\u4ef0\u5417\ud83d\ude2d\uff09</p>"},{"location":"Diary/days_challenge/24.11/#2024116","title":"2024.11.6:","text":"<p>Things have done</p> <p>\u80cc\u5355\u8bcd/R60/20min \u4e0a\u8bfe/physics/100min \u535a\u5ba2/new css/25min \u8ba1\u7ec4/Lab4/120min \u4f11\u606f/noon break/60min ADS/HW/95min \u8fd0\u52a8/NM/40min \u80cc\u5355\u8bcd/R100/25min \u8ba1\u7ec4/HW/70min \u535a\u5ba2/Music/25min \u79d1\u7814/read paper/80min \u6982\u7387\u8bba/HW/30min \u6982\u7387\u8bba/review/35min \u603b\u7ed3/blog/20min</p> <p>Pie Chart:(1)</p> Introspection <p>\u4eca\u5929\u597d\u50cf\u786e\u5b9e\u5f88\u96be\u56de\u5fc6\u8d77\u6709\u4ec0\u4e48\u5927\u6bb5\u65f6\u95f4\u7684\u6d6a\u8d39\uff0c\u4f46\u611f\u89c9\u6548\u7387\u8fd8\u662f\u4e0d\u9ad8\uff0c\u800c\u4e14\u7531\u4e8e\u8ba4\u8bc6\u5230\u4e86\u81ea\u5df1\u7684\u5f31\u5c0f\uff0c\u73b0\u5728\u975e\u5e38\u7d27\u5f20\u3002\u4e3a\u4ec0\u4e48\u6211\u773c\u89d2\u5e38\u6302\u7740\u6cea\u82b1\uff1f\u4e3a\u4ec0\u4e48\u4e3b\u52a8\u6216\u81ea\u52a8\u5730\u884c\u52a8\u53d8\u5feb\uff1f\u4e8b\u60c5\u592a\u591a\u4e86\uff0c\u4f46\u6211\u4e00\u5b9a\u8981\u5b8c\u6210\u5927\u4e00\u7edf\uff01\u6211\u4e0d\u4f1a\u518d\u7ba1\u4efb\u4f55\u4e0e\u6211\u65e0\u5173\u7684\u4eba\u4e0e\u4e8b\uff0c\u6211\u73b0\u5728\u53ea\u5173\u6ce8\u5982\u4f55\u9ad8\u6548\u524d\u8fdb\u3002\u5f53\u4f60\u75db\u82e6\u6d41\u6cea\u65f6\uff08\u4f46\u5176\u5b9e\u4f60\u6ca1\u6709\u65f6\u95f4\u4f24\u5fc3\uff09\uff0c\u4e0e\u4efb\u4f55\u4eba\u503e\u8bc9\u5176\u5b9e\u7686\u65e0\u7528\uff0c\u53ea\u6709\u7f8e\u597d\u7684\u4fe1\u4ef0\u4f1a\u966a\u4f34\u7740\u4f60\uff0c\u552f\u6709\u54ac\u7259\u575a\u6301\u5ea6\u8fc7\uff0c\u624d\u6709\u5411\u4e0a\u7684\u672a\u6765\uff0c\u624d\u5bf9\u5f97\u8d77\u5fc3\u4e2d\u7684\u4fe1\u4ef0\uff01  </p> <p>I need time! I won't care anyone or anything irrelevant! But only having time is not sufficient,I also need effeciency! Belief! Thank you for accompanying me! It is impossible for me to fully repay you for your kindness! What I can is to show you that I will spare no effort to live up to your expectation! Since the passed one second!</p>"},{"location":"Diary/days_challenge/24.11/#2024117","title":"2024.11.7:","text":"<p>Things have been done</p> <p>\u6982\u7387\u8bba/review/80min \u80cc\u5355\u8bcd/R100/20min AI\u5bfc/Lab/60min \u4e0a\u8bfe/\u5b9a\u5411\u8d8a\u91ce/100min \uff08\u80cc\u5355\u8bcd/R110/parallel\uff09 \u8fd0\u52a8/NM/20min \u4f11\u606f/noon break/20min \u4e0a\u8bfe/NA/100min \uff08AI\u5bfc/Lab/parallel+25min\uff09 \u80cc\u5355\u8bcd/R90/25min \u79d1\u7814/RP/90min \u6982\u7387\u8bba/HW/30min \u7269\u7406/HW/15min \u6982\u7387\u8bba/HW/15min \u535a\u5ba2/conclusion/20min</p> <p>Pie Chart:(1)</p> Introspection <p>\u597d\u5bb6\u4f19\uff0c\u8fd9\u4e0b\u4eca\u5929\u6709\u5927\u6bb5\u65f6\u95f4\u7684\u6d6a\u8d39\uff08\u82e6\u7b11\uff09\u3002Belife \u7684\u201c\u9634\u9633\u8f6c\u6362\u201d\u8fd8\u662f\u5f88\u53d7\u7528\u7684\uff0c\u6bd5\u7adf\u8c6a\u8a00\u58ee\u8bed\u8c01\u90fd\u80fd\u8bf4\uff0c\u4f46\u4e00\u4e2a\u4eba\u771f\u6b63\u8ba4\u771f\u505a\u4e86\u4ec0\u4e48\uff0c\u624d\u662f\u6210\u529f\u7684\u80dc\u8d1f\u624b......\u4eca\u5929\u5c31\u4e0d\u591a\u8a00\u4e86\uff0c\u65e9\u7761\uff0c\u660e\u5929\u5c06\u662f\u53c8\u4e00\u6b21\u8bd5\u70bc\u3002</p>"},{"location":"Diary/days_challenge/24.8/","title":"24.8","text":""},{"location":"Diary/days_challenge/24.8/#2024815","title":"2024.8.15:","text":"<p>Things have done</p> <p>\u6668\u8dd1/normal/10min \u6668\u8bfb/C1SC/30min \u80cc\u5355\u8bcd/review40/8min \u5b66\u7269\u7406/C4S1/55min \u5b66\u7269\u7406/C4S2&amp;C4S3(Q)/15min \u8ba1\u79d1\u5bfc/C13&amp;C14(Q)/20min \u80cc\u5355\u8bcd/R29N20/20min \u63a2\u7d22/mkdocs&amp;latex/170min \u63a2\u7d22/latex/180min \u63a2\u7d22/visualization/115min </p> <p>Pie Chart:(1)</p> Introspection <p>\u4eca\u5929\u5927\u90e8\u5206\u65f6\u95f4\u82b1\u5728\u4e86\u201c\u63a2\u7d22\u201d\u4e0a\uff0c\u809d\u5230\u4e86\u51cc\u6668\uff0c\u96be\u4ee5\u81ea\u62d4\u3002\u8fd8\u662f\u5e94\u8be5\u591a\u82b1\u65f6\u95f4\u5728\u6253\u5b66\u4e1a\u57fa\u7840\u4e0a\uff0c\u4ee5\u540e\u8981\u5c3d\u91cf\u505a\u5230\u201c\u8364\u7d20\u642d\u914d\u201d\u3002\u65f6\u95f4\u7684\u7ba1\u63a7\u4e5f\u9700\u8981\u52a0\u5f3a\uff08\u51cc\u6668\u4f11\u606f\u8fd8\u662f\u4e0d\u53ef\u6301\u7eed\u7684\uff09==</p>"},{"location":"Diary/days_challenge/24.8/#2024816","title":"2024.8.16:","text":"<p>Things have done</p> <p>\u6668\u8dd1/normal/10min \u63a2\u7d22/?/?min \u8fd0\u52a8/normal/20min \u63a2\u7d22/?/?min</p> Introsperction <p>\u6211\u5931\u8d25\u4e86==,\u65f6\u95f4\u4e3b\u8981\u82b1\u5728\u63a2\u7d22html\u4e0eLatex\u516c\u5f0f\u989c\u8272\u4e0a\uff0c\u660e\u5929\uff0c\u52a0\u6cb9\u6539\u8fdb...</p>"},{"location":"Diary/days_challenge/24.8/#2024817","title":"2024.8.17:","text":"<p>Things have done</p> <p>\u6668\u8dd1/normal/10min \u6668\u8bfb/C2SA/15min \u80cc\u5355\u8bcd/R40/8min \u5b66\u7269\u7406/C4S4/15min \u63a2\u7d22/?/?min</p> Introsperction <p>emm...\u65e0\u9700\u591a\u8a00\uff0c\u63a2\u7d22\u4e86\u97f3\u9891\u5904\u7406\u3001html\u3001markdown\u5143\u7d20\u63d2\u5165(\u5982 https://xuebastudy.github.io/notebook/Music/Others/mp3_visualization/)\uff0c\u7f51\u7ad9\u6e90\u4ee3\u7801\u4e0e\u7ed3\u6784\uff0c\u8def\u5f84\u4f7f\u7528...\u867d\u7136\u63a2\u7d22\u6682\u65f6\u96be\u4ee5\u904f\u5236\uff0c\u4f46\u660e\u5929\u5f00\u59cb\u8981\u4ed4\u7ec6\u8bb0\u5f55\u4e00\u4e0b\uff0c\u628a\u65e0\u610f\u95f4\u6d41\u5931\u7684\u65f6\u95f4\u7528\u5230\u5b66\u4e1a\u4e0a...\u6bd5\u7adf\u5e72\u5f97\u597d\uff0c\u624d\u50cf\u4e2a \"believer\" ...\u54ce\u5440\u5440,\u6bd5\u7adf\u4f0a\u857e\u5a1c\u5c0f\u59d0\u5b9e\u5728\u592a\u53ef\u7231\u5566!\ud83d\ude0a...</p>"},{"location":"Diary/days_challenge/24.8/#2024818","title":"2024.8.18:","text":"<p>Things have done</p> <p>\u6668\u8dd1/normal/10min \u6668\u8bfb/C2SB/30min \u63a2\u7d22/C\u76d8\u7a7a\u95f4&amp;\u8f6f\u4ef6\u5220\u9664/120min \u6574\u7406/\u7535\u8111\u6587\u4ef6/180min \u6574\u7406/\u5fae\u4fe1\u6587\u4ef6/20min \u5c0f\u63d0\u7434/task&amp;free/60min \u6574\u7406/\u5fae\u4fe1\u6587\u4ef6/30min</p> Introsperction <p>\u5982\u4e0a\uff0c\u6211\u7684\u8bc4\u4ef7\u662f\u5e94\u8be5\u628a\u975e\u5b66\u4e1a\u4e8b\u52a1\u653e\u5165\u201c\u89c4\u5b9a\u597d\u65f6\u95f4\u7684\u4f11\u606f\u201d\uff0c\u5426\u5219\u5bb9\u6613\u4e0a\u5934\uff0c\u5206\u4e0d\u6e05\u4e3b\u6b21\u5173\u7cfb...\u4f0a\u857e\u5a1c\u5c0f\u59d0\uff0c\u60a8\u613f\u610f\u7ed9\u6211\u4e00\u6b21\u6539\u8fdb\u7684\u673a\u4f1a\u5417\ud83d\ude2d...</p>"},{"location":"Diary/days_challenge/24.8/#2024819","title":"2024.8.19:","text":"Introsperction <p>emo\u4e86...\u4e00\u4e0d\u5c0f\u5fc3\u770b\u4e86Re0\u7684\u7b2c\u4e00\u5b63\u548c\u90e8\u5206\u7b2c\u4e8c\u5b63(\u65e9\u6709\u8033\u95fb\uff0c\u4f46\u6ca1\u88ab\u5267\u900f)\u3002\u597d\u4e45\u6ca1\u6709\u8fc7\u201c\u5fc3\u98a4\u201d(\u7269\u7406\u610f\u4e49\u4e0a)\u7684\u611f\u89c9\u4e86\uff0c\u4e0d\u5f97\u4e0d\u8bf4\uff0cRe0\u5bf9\u6c1b\u56f4\u7684\u4e00\u6b65\u6b65\u6e32\u67d3\u5b9e\u5728\u592a...\u67d0\u4e9b\u573a\u666f\u4e0b\u7684\u7edd\u671b\u611f\u4e0e\u65e0\u52a9\u611f...\u867d\u7136\u786e\u5b9e\u6709\u6570\u6b21\u4e00\u8d25\u6d82\u5730\u540e\u6293\u4f4f\u5e0c\u671b\u7a81\u7834\u7684\u5267\u60c5\uff0c\u4f46\u8fd8\u662f\u63a5\u8fde\u4e0d\u65ad\u5730\u5904\u4e8e\u7d27\u5f20\u4e8b\u4ef6\u4e2d\uff0c\u867d\u7136\u53d1\u7cd6\u4e5f\u6709(\u96ea\u4e2d\u9001\u78b3\u5f0f\uff0c\u66f4\u52a0\u73cd\u8d35)\uff0c\u4f46\u4e00\u60f3\u5230\u4e4b\u540e\u7684\u201c\u5200\u201d\u5267\u60c5\uff0c\u53c8\u96be\u4ee5\u4e0b\u54bd\u4e86\u3002\u770b\u8fc7\u540e\u4e5f\u4e0d\u81ea\u89c9\u5730\u601d\u8003\u8fc7\u5f88\u4e45\uff0c\u611f\u89c9\u786e\u5b9e\u662f\u795e\u4f5c\uff0c\u60b2\u5267\u786e\u5b9e\u6709\u6d17\u6da4\u4eba\u5fc3\u7684\u529b\u91cf\uff0c\u4f46\u6709\u70b9\u8ba9\u6211\u5598\u4e0d\u8fc7\u6c14\u6765\u3002 Re0\u76ee\u524d\u6211\u4e0d\u5fcd\u518d\u770b\uff0c\u8fd8\u662f\u8ba9\u4e00\u4e9b\u6e29\u60c5\u3001\u6b22\u5feb(\u54ea\u6015\u6df1\u5ea6\u4e00\u822c)\u7684\u756a\u6765\u6cbb\u6108\u5076\u5c14\u75b2\u60eb\u7684\u6211\u5427\u3002\u4f46\u603b\u800c\u8a00\u4e4b\uff0cRe0\u5df2\u7ecf\u5728\u6211\u5fc3\u4e2d\u7559\u4e0b\u4e86\u201c\u60ca\u9e3f\u4e00\u77a5\u201d\uff0c\u5f71\u54cd\u5f88\u5927...</p>"},{"location":"Diary/days_challenge/24.8/#2024820","title":"2024.8.20:","text":"<p>Things have done</p> <p>\u80cc\u5355\u8bcd/R60/12min \u5b66\u7269\u7406/C5S3&amp;4&amp;5/45min \u5b66\u7269\u7406/C5S6/20min \u6c47\u7f16\u8bed\u8a00/C11-17(L1)/15min \u8ba1\u79d1\u5bfc/C15/15min \u83dc\u9e1f\u6559\u7a0b/\u6b63\u5219\u8868\u8fbe\u5f0f1/30min \u83dc\u9e1f\u6559\u7a0b/\u6b63\u5219\u8868\u8fbe\u5f0f2/20min</p> Introsperction <p>\u5f88\u6709\u8da3\u54df~...\u51fa\u53bb\u505a\u5ba2\u7684\u65f6\u5019\u5728\u7a7a\u95f2\u65f6\u95f4\u4e89\u5206\u593a\u79d2\uff0c\u4e0d\u81ea\u89c9\u5730\u5b66\u8d77\u4e86\u65b0\u77e5\u8bc6\uff08\u6b63\u5219\u8868\u8fbe\u5f0f\uff09\uff0c\u5728\u5bb6\u91cc\u53cd\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u751a\u81f3\u4e00\u4e8b\u65e0\u6210\u3002\u771f\u662f\u5947\u602a\u7684\u4eba\u6027\u634f\uff08\u82e6\u7b11...\u8fd9\u79cd\u4e8b\u4e0d\u50cf\u5fc3\u60c5\u4f4e\u843d\uff0c\u53ef\u4ee5\u5bc4\u6258\u5185\u5fc3\u7ed9\u4fe1\u4ef0\u3002\u600e\u4e48\u529e\u634f...</p>"},{"location":"Diary/days_challenge/24.8/#2024821","title":"2024.8.21:","text":"Introsperction <p>\uff08\u7a7a\u767d\uff09</p>"},{"location":"Diary/days_challenge/24.8/#2024822","title":"2024.8.22:","text":"<p>Things have done</p> <p>\u6668\u8dd1/normal/10min \u6668\u8bfb/C3SA/30min \u5b66\u7269\u7406/C5S7/30min \u80cc\u5355\u8bcd/R80/20min \u5b66\u7269\u7406/C5S8/30min \u5b66\u7269\u7406/C5S9(Q)/25min \u535a\u5ba2/write/35min \u8ba1\u79d1\u5bfc/C16/30min \u5c0f\u63d0\u7434/task&amp;free/80min GAMES101/homework_1/40min</p> Introsperction <p>\u50cf\u662f\u505a\u4e86\u4e00\u70b9\u4e8b\uff0c\u4f46\u8fd8\u662f\u4e0d\u591f\uff0c\u65e0\u8bba\u662f\u5bf9\u4fe1\u4ef0\u8fd8\u662f\u5bf9\u81ea\u5df1\u6765\u8bf4...\u8fd8\u6709\u4e00\u4e2a\u611f\u60f3\uff0c\u8981\u5c3d\u91cf\u4efb\u52a1\u9a71\u52a8\u5f0f\u5b66\u4e60\uff0c\u8fd9\u6837\u5f80\u5f80\u66f4\u5bb9\u6613\u5728\u6709\u9650\u65f6\u95f4\u6293\u4f4f\u91cd\u70b9\uff0c\u6bd5\u7adf\u4e1c\u897f\u662f\u5b66\u4e0d\u5b8c\u7684\uff0c\u5148\u6293\u5927\u653e\u5c0f\u624d\u7b26\u5408\u5b9e\u9645\u4f18\u89e3\uff0c\u4e0d\u53ef\u4e3a\u4e86\u6240\u8c13\u7684\u201c\u5b8c\u6574\u611f\u201d\u7a7a\u8017\u65f6\u95f4...</p>"},{"location":"Diary/days_challenge/24.8/#2024823","title":"2024.8.23:","text":"Introsperction <p>\u5927\u5b66\u82f1\u8bed\u516d\u7ea7\u88f8\u8003569\uff0c\u672c\u6765\u8fd8\u81ea\u6211\u611f\u89c9\u826f\u597d\uff0c\u7ed3\u679c\u8ba4\u8bc6\u7684\u540c\u5b66\u4e2a\u4e2a550+\uff0c\u6709\u70b9emo\u4e86...\u7b97\u4e86\uff0c\u518d\u63a5\u518d\u5389\uff0c\u4e0b\u4e00\u6b21\u76ee\u6807\u662f600+ \uff01\uff01</p>"},{"location":"Diary/days_challenge/24.8/#2024829","title":"2024.8.29:","text":"Introsperction <p>\u9761\u4e0d\u6709\u521d\uff0c\u9c9c\u514b\u6709\u7ec8\u3002\u5982\u679c\u4f60\u770b\u5230\u4e86\u8fd9\u91cc\uff0c\u8fd9\u662f\u6211\u7ed9\u4f60\u7684\u544a\u8beb\u3002\u6211\u7684\u5047\u671f\u5df2\u7136\u7ed3\u675f\uff0c\u660e\u5929\u662f\u65b0\u7684\u5f00\u59cb\u3002</p>"},{"location":"Lessons/How_1/","title":"How to learn","text":"<p>Abstract</p> <p>\u4ecb\u7ecd\u4e86\u7b14\u8005\u5728\u5927\u4e8c\u4e0a\uff0c\u4e00\u4e9b\u6821\u5185\u8bfe\u7a0b\u7684\u5b66\u4e60\u6d41\u7a0b\u3002</p>"},{"location":"Lessons/How_1/#na","title":"NA","text":"<ul> <li>\u5728\u5b9e\u4f8b\u4e2d\u68c0\u9a8c\u4f3c\u61c2\u975e\u61c2\u7684\u77e5\u8bc6</li> <li>\u5386\u5e74\u5377 good</li> <li>Notebook : jiepeng / Jianjun Zhou's </li> </ul> <p>Pre:  Notebook \u2014\u2014 \u9ec4PPT  </p> <p>Dur:  Notebook &amp; \u9ec4PPT &amp; \u767dPPT  </p> <p>Aft:  \u8bfe\u4e0a\u7591\u60d1\u3010Dur*3\u3011\u2014\u2014 \u4f5c\u4e1a\uff08 \u2014\u2014 Lab\uff09  </p> <p>Rev:  Notebook &amp; \u767dPPT \u2014\u2014 \u9ec4PPT &amp; \u5386\u5e74\u5377 \uff08 \u2014\u2014 \u4f5c\u4e1a\uff09  </p> <p>More:  \u4e66\u672c\u3010\u8d2f\u901a\u7406\u89e3\u3011\u2014\u2014 \u7b97\u6cd5\u5177\u4f53\u4ee3\u7801  </p>"},{"location":"Lessons/How_1/#physics-iih","title":"Physics II\uff08H\uff09","text":"<p>Pre:  \u300a\u65b0\u6982\u5ff5\u7269\u7406\u300b(\u8bb2\u89e3)  </p> <p>Dur:  \u63d0\u70bcPPT &amp; \u300a\u65b0\u6982\u5ff5\u7269\u7406\u300b(\u8bb2\u89e3&amp;\u4e60\u9898)  </p> <p>Aft:  (\u63d0\u70bcPPT \u2014\u2014) \u4f5c\u4e1a \u2014\u2014 \u300a\u65b0\u6982\u5ff5\u7269\u7406\u300b(\u8bb2\u89e3&amp;\u4e60\u9898)  </p> <p>Rev:  PPT \u2014\u2014 \u81ea\u884c\u603b\u7ed3\u7b14\u8bb0  &amp;\u300a\u65b0\u6982\u5ff5\u7269\u7406\u300b \u2014\u2014 \u5386\u5e74\u5377  </p>"},{"location":"Lessons/How_1/#ads","title":"ADS","text":"<ul> <li>\u52ff\u5fd8\u968f\u65f6\u52a0\u5165note</li> </ul> <p>Pre:  Notebook</p> <p>Dur:  Notebook</p> <p>Aft:  PPT &amp; Notebook \u2014\u2014 \u4ed6\u4eba\u4f5c\u4e1a [JJZ] ( \u2014\u2014 \u4e66) \u2014\u2014 PTA \u2014\u2014 \u8ba2\u6b63\u4e0a\u6b21PTA\uff08\u52ff\u5fd8\u5bf9\u7b54\u6848\uff09</p> <p>Rev (phased):  PPT &amp; Notebook \u2014\u2014 PTA \u2014\u2014 \u5386\u5e74\u5377(\u7f51\u4e0a&amp;\u7ebf\u4e0b) ( \u2014\u2014 \u4ee3\u7801\u5b9e\u73b0)</p> <p>More:  \u4e66 &amp; \u7f51\u4e0a\u627e\u66f4\u591a\u8d44\u6599\u81ea\u5b66 \u2014\u2014 Leetcode\u5b9e\u6218</p>"},{"location":"Lessons/How_1/#co","title":"CO","text":"<p>Pre: mooc</p> <p>Dur: PPT\u4f5c\u6ce8\u91ca</p> <p>Aft: PPT (&amp; \u9a6c\u5fb7\u667a\u4e91) \u2014\u2014 Notebook \u2014\u2014 \u4f5c\u4e1a ( \u2014\u2014 Lab)</p> <p>Rev:</p>"},{"location":"Lessons/Math/ODE/note/","title":"\u5e38\u5fae\u5206\u65b9\u7a0b","text":""},{"location":"Lessons/Math/ODE/note/#_1","title":"\u5e38\u5fae\u5206\u65b9\u7a0b","text":"<p>Abstract</p> <p>\u6765\u81ea\u6d59\u6c5f\u5927\u5b66\u8bfe\u7a0b\u653b\u7565\u5171\u4eab\u8ba1\u5212\u7684\u4e00\u4e9b\u7b14\u8bb0\uff0c\u6db5\u76d6\u4e86\u57fa\u7840\u5e38\u89c1\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u8ba1\u7b97\uff0c\u6709\u89e3\u6cd5\u4e0e\u4f8b\u9898</p>"},{"location":"Lessons/Math/ODE/note/#_2","title":"\u89e3\u6cd5","text":""},{"location":"Lessons/Math/ODE/note/#_3","title":"\u4f8b\u9898\uff08\u4e00\u9636\u3001\u5168\u5fae\u5206\u65b9\u7a0b\uff09","text":""},{"location":"Lessons/Math/ODE/note/#_4","title":"\u4f8b\u9898\uff08\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\uff09","text":""},{"location":"Lessons/Math/ODE/note/#_5","title":"\u4f8b\u9898\uff08\u5fae\u5206\u65b9\u7a0b\u7ec4\uff09","text":""},{"location":"Music/Others/mp3_visualization/","title":"mp3->mp4(\u6709\u58f0\u9891\u8c31)","text":"<p>Abstract</p> <p>1.\u672c\u9879\u76ee\u6838\u5fc3\u6765\u6e90\u4e8eCTF\u6bd4\u8d5b\uff1aHackergame 2021 \u7684 p\ud83d\ude2dq  (generate_sound_visualization.py) 2.\u539f\u59cb\u52a8\u673a\u5728\u4e8e\u5e0c\u671b\u4ed6\u4eba\u613f\u610f\u6b23\u8d4f\u9b54\u5973\u4e4b\u65c5\u7684ED(piano) 3.\u4e3b\u8981\u4f7f\u7528\u4e86 python \u4e0e ffmpeg \u8fdb\u884c\u5904\u7406</p>"},{"location":"Music/Others/mp3_visualization/#1-python-librosa-mp3-gif","title":"1. \u4f7f\u7528 python \u97f3\u9891\u5904\u7406\u5e93 librosa \u5c06 mp3\u6587\u4ef6 \u8f6c\u6362\u4e3a gif\u9891\u8c31\u52a8\u56fe","text":"<p>(\u6587\u4ef6\u8f83\u5927\u65f6\u9700\u8981\u4e00\u5b9a\u65f6\u95f4) </p>Python<pre><code>from array2gif import write_gif  # version: 1.0.4\nimport librosa  # version: 0.10.2.post1\nimport numpy  # version: 1.26.4\n\nnum_freqs = 32\nquantize = 2\nmin_db = -60\nmax_db = 30\nfft_window_size = 2048\nframe_step_size = 512\nwindow_function_type = 'hann'\ncolor_pixel = (30, 144, 255)  # DodgerBlue\nwhite_pixel = (255, 255, 255)  # Colors etc. can be modified by yourself\n\ndef convert_to_gif(src, dst):\n    y, sample_rate = librosa.load(src)\n\n    spectrogram = numpy.around(\n        librosa.power_to_db(\n            librosa.feature.melspectrogram(\n                y=y, sr=sample_rate, n_mels=num_freqs,\n                n_fft=fft_window_size,\n                hop_length=frame_step_size,\n                window=window_function_type\n            )\n        ) / quantize\n    ) * quantize\n\n    gif_data = [\n        numpy.kron(\n            numpy.array([\n                [\n                    color_pixel if freq % 2 and i &lt; round(\n                        frame[freq // 2]) else white_pixel\n                    for i in reversed(\n                        range(min_db, max_db + 1, quantize))\n                ]\n                for freq in range(num_freqs * 2 + 1)\n            ]),\n            numpy.ones([quantize, quantize, 1])\n        )\n        for frame in spectrogram.transpose()\n    ]\n\n    write_gif(gif_data, dst, fps=sample_rate/frame_step_size)\n\nif __name__ == '__main__':\n    convert_to_gif('input.mp3', 'output.gif')  # sample rate is 22050 Hz\n</code></pre>"},{"location":"Music/Others/mp3_visualization/#2-ffmpeg-mp3-gif-mp4","title":"2. \u4f7f\u7528 ffmpeg \u5c06 mp3\u6587\u4ef6 \u4e0e gif\u6587\u4ef6 \u5408\u6210\u4e3a mp4\u6587\u4ef6","text":"<ul> <li> <p>\u5728\u547d\u4ee4\u884c\u7528<code>ffmpeg -i input.mp3</code> \u4e0e <code>ffmpeg -i output.gif</code> \u5206\u522b\u6c42\u51fa\u65f6\u957f\uff1aduration_of_mp3 \u3001duration_of_gif  </p> </li> <li> <p>\u4f7f\u7528 <code>ffmpeg -i output.gif -i input.mp3 -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2, fps=25, setpts=PTS*(duration_of_mp3/duration_of_gif)\" -c:v libx264 -pix_fmt yuv420p -c:a aac -b:a 192k -shortest output.mp4</code> \u547d\u4ee4\u751f\u6210 mp4\u6587\u4ef6 (\u547d\u4ee4\u4e2d\u7684 duration_of_mp3/duration_of_gif \u9700\u8981\u6839\u636e\u7b2c\u4e00\u6b65\u4fee\u6539\uff0c\u5982 223/192 )</p> </li> </ul> \u53c2\u6570\u89e3\u91ca <ul> <li>scale=trunc(iw/2)*2:trunc(ih/2)*2\uff1a\u786e\u4fdd\u89c6\u9891\u7684\u5bbd\u9ad8\u662f\u5076\u6570\uff0c\u4ee5\u907f\u514d\u7f16\u7801\u95ee\u9898\u3002</li> <li>fps=25\uff1a\u8bbe\u7f6e\u76ee\u6807\u5e27\u7387\u4e3a 25 FPS\uff08\u4f60\u53ef\u4ee5\u8c03\u6574\u8fd9\u4e2a\u503c\uff09\u3002</li> <li>setpts=PTS*(duration_of_mp3/duration_of_gif)\uff1a\u81ea\u52a8\u8ba1\u7b97\u5e76\u8c03\u6574\u5e27\u65f6\u95f4\u6233\uff0c\u4ee5\u4f7f GIF \u64ad\u653e\u65f6\u957f\u7b49\u4e8e MP3 \u7684\u65f6\u957f\u3002</li> <li>-c:v libx264\uff1a\u4f7f\u7528 H.264 \u7f16\u7801\u5668\u538b\u7f29\u89c6\u9891\u3002</li> <li>-pix_fmt yuv420p\uff1a\u8bbe\u7f6e\u50cf\u7d20\u683c\u5f0f\u4e3a yuv420p\u3002</li> <li>-c:a aac\uff1a\u4f7f\u7528 AAC \u7f16\u7801\u5668\u538b\u7f29\u97f3\u9891\u3002</li> <li>-b:a 192k\uff1a\u8bbe\u7f6e\u97f3\u9891\u6bd4\u7279\u7387\u4e3a 192 kbps\u3002</li> <li>-shortest\uff1a\u786e\u4fdd\u89c6\u9891\u7684\u65f6\u957f\u4e0d\u8d85\u8fc7 MP3 \u7684\u65f6\u957f\u3002</li> </ul>"},{"location":"Music/Others/mp3_visualization/#3","title":"3. \u6548\u679c\u5c55\u793a","text":"The sound is from \u300a\u9b54\u5973\u306e\u65c5\u3005\u300bED"},{"location":"Music/places/","title":"Music with places","text":""},{"location":"Music/places/#music-with-places","title":"Music with places","text":"<p>Abstract</p> <ol> <li> <p>\u300a\u6211\u4eec\u4ecd\u672a\u77e5\u9053\u90a3\u5929\u6240\u770b\u89c1\u7684\u82b1\u7684\u540d\u5b57\u300b\u5373\u300a\u672a\u95fb\u82b1\u540d\u300b\uff0c\u50cf\u8fd9\u6837\u7684\u9057\u61be\u6c38\u8fdc\u4f34\u968f\u7740\u6211\u4eec\u3002  </p> </li> <li> <p>\u4e5f\u8bb8\u67d0\u4e9b\u5730\u65b9\u3001\u573a\u666f\u4e0e\u67d0\u4e9b\u97f3\u4e50\u5171\u540c\u89e6\u52a8\u4e86\u4f60\uff0c\u8ba9\u4f60\u201c\u5fc3\u6d41\u201d\uff0c\u4e45\u4e45\u65e0\u6cd5\u5fd8\u6000\uff0c\u4e8b\u540e\u5374\u96be\u4ee5\u5bfb\u627e\u5230\u90a3\u9996\u6b4c\u7684\u540d\u5b57\uff08\u5f53\u7136\uff0c\u6709\u65f6\u662f\u4e0d\u8bb0\u5f97\u5730\u70b9\uff09\uff0c\u5373\u4f7f\u627e\u5230\u4e86\uff0c\u4e5f\u5f80\u5f80\u96be\u4ee5\u590d\u523b\u5f53\u65f6\u7684\u611f\u53d7\u3002  </p> </li> <li> <p>\u5509\uff0c\u751f\u6d3b\u603b\u662f\u5145\u6ee1\u4e86\u9057\u61be\uff0c\u5373\u4f7f\u6211\u4eec\u60f3\u8981\u8bb0\u5f55\u4e0b\u6765\uff0c\u4e5f\u603b\u662f\u7edd\u5927\u90e8\u5206\u6700\u7ec8\u6d88\u901d\u3002  </p> </li> <li> <p>\u90a3\u4e9b Music with places \u554a\uff0c\u90a3\u4e9b\u66f4\u5e7f\u6cdb\u7684\u751f\u6d3b\u9057\u61be\u554a\uff0c\u6211\u4eec\u53c8\u80fd\u600e\u4e48\u529e\u5462\uff1f  </p> </li> <li> <p>Roxy \u66fe\u5b89\u6170 Rudeus Greyrat\uff1a\u201c\u5982\u679c\u4ee5\u524d\u7684\u9057\u61be\u65e0\u6cd5\u633d\u56de\u7684\u8bdd\uff0c\u90a3\u53ea\u597d\u597d\u597d\u73cd\u60dc\u5f53\u4e0b\u4e86\u3002\u201d  </p> </li> <li> <p>\u65e2\u7136\u4e0d\u53ef\u80fd\u5168\u8bb0\u4e0b\u6765\uff0c\u90a3\u5c31\u96f6\u661f\u5730\u8bb0\u8bb0\u5427\u3002  </p> </li> <li> <p>\u8fd8\u6709\uff0c\u522b\u592a\u611f\u4f24\u4e86\uff0c\u8d76\u7d27\u52aa\u529b\u8d77\u6765\uff0c\u4fe1\u4ef0\u4e0e\u4f60\u540c\u5728\u3002  </p> </li> </ol>"},{"location":"Music/places/ZJU_Mainlib/","title":"\u6d59\u5927\u4e3b\u56fe\u4e4b\u591c","text":"<p>Abstract</p> <ol> <li>\u8fd9\u662f\u6d59\u6c5f\u5927\u5b66\u7d2b\u91d1\u6e2f\u6821\u533a\u4e3b\u56fe\u4e66\u9986\u7684\u95ed\u9986\u97f3\u4e50\uff0822:15 - 22:30 pm\uff09\uff0c\u5df2\u6392\u5e8f\uff08\u7b2c\u516d\u9996\u4ecd\u4e3a\u201c\u7b2c\u4e8c\u9996\u201d\uff09\uff0c\u5e38\u5e38\u8fd12000\u4e2a\u4f4d\u7f6e\u5ea7\u65e0\u865a\u5e2d\u3002  </li> <li>\u5176\u5b9e\u5b66\u4e60\u4e5f\u4e0d\u5fc5\u5728\u56fe\u4e66\u9986\uff0c\u5f53\u7136\u66f4\u4e0d\u5fc5\u71ac\u5230\u95ed\u9986\u4ee5\u5f70\u663e\u81ea\u5df1\u7684\u52aa\u529b\u3002  </li> <li>\u5982\u679c\u865a\u5ea6\u4e86\u5149\u9634\uff0c\u5f88\u53ef\u80fd\u5230\u5934\u6765\u7126\u8651\u4e0d\u5b89\uff0c\u542c\u95ed\u9986\u97f3\u4e50\u4e0d\u4f1a\u6539\u53d8\u4f60\u672a\u52aa\u529b\u7684\u4e8b\u5b9e\uff0c\u7f13\u89e3\u7126\u8651\u66f4\u662f\u65e0\u7a3d\u4e4b\u8c08\u3002\uff08\u751a\u81f3\u53ef\u80fd\u7531\u4e8e\u53cd\u5dee\uff0c\u66f4\u52a0\u7a7a\u6d1e\u8ff7\u832b\uff09 \uff08\u591a\u6101\u5584\u611f\u3001\u80e1\u601d\u4e71\u60f3\u3001\u7126\u8651\u81ea\u6ee1\uff0c\u90fd\u662f\u201c\u95f2\u201d\u51fa\u6765\u7684\uff0c\u4e0d\u662f\u201c\u5fd9\u201d\u51fa\u6765\u7684\uff09 </li> <li>\u82e5\u6709\u4e86\u8ba4\u771f\u5b66\u4e60\u3001\u94bb\u7814\u95ee\u9898\u540e\u7684\u5145\u5b9e\u7684\u5e73\u9759\u611f\uff0c\u5176\u5b9e\u4e5f\u4e0d\u5fc5\u542c\u95ed\u9986\u97f3\u4e50\uff0c\u8fd9\u53ea\u662f\u4e00\u79cd optional \u9526\u4e0a\u6dfb\u82b1\u3002  </li> <li>\u90a3\u4e48\u8fd9\u4e9b\u6b4c\u6574\u7406\u51fa\u6765\uff08\u6211\u901a\u8fc7\u5f55\u97f3\u3001App\u201c\u542c\u97f3\u8fa8\u66f2\u201d\uff09\u6709\u4ec0\u4e48\u7528\u5462\uff1f\u5927\u6982\u5c31\u662f\u4e3a\u4e86\u886c\u6258\u4e0a\u8ff0\u601d\u8003\u5427\uff08\ud83e\udd13\ud83d\udc46\uff09  </li> </ol> <p>\u591c\u665a\u7684\u56fe\u4e66\u9986\u5916\u526a\u5f71: (1) (2)</p> <ol> <li></li> <li></li> </ol>"},{"location":"Paper/Quantization/1/","title":"(Survey) Model Quantization for DNN in Image Classification","text":""},{"location":"Paper/Quantization/1/#_1","title":"(Survey) Model Quantization for DNN in Image Classification","text":""},{"location":"Paper/Quantization/1/#paper-a-comprehensive-survey-on-model-quantization-for-deep-neural-networks-in-image-classification","title":"Paper: A Comprehensive Survey on Model Quantization for Deep Neural Networks in Image Classification","text":"<p>Abstract</p> <ul> <li>\u80cc\u666f\uff1aDeep Neural Networks (DNNs) \u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u6b65\uff0c\u4f18\u70b9\u662f\u7cbe\u5ea6\u9ad8\uff0c\u7f3a\u70b9\u662f\u5b58\u50a8\u6d88\u8017\u9ad8\uff0c\u8017\u80fd\u9ad8\uff0c\u8fd9\u4f7f\u5f97\u4ed6\u4eec\u5728\u6709\u9650\u7684\u786c\u4ef6\u8d44\u6e90\u4e0a\u96be\u4ee5\u4f7f\u7528\uff0c\u91cf\u5316 \uff08Quantization\uff09\uff08the full-precision values are stored in low bit-width precision\uff09\u5373\u4e3a\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u7684\u65b9\u6cd5\u4e4b\u4e00\u3002</li> <li>Quantization\u7684\u4f18\u70b9\uff1a1. Quantization not only reduces memory requirements but also replaces high-cost operations with low-cost ones. 2. DNN quantization offers flexibility and efficiency in hardware design, making it a widely adopted technique in various methods.</li> <li>\u8d21\u732e\uff1a<ol> <li>Consequently, we present a comprehensive survey of quantization concepts and methods, with a focus on image classification. </li> <li>We describe clustering-based quantization methods and explore the use of a scale factor parameter for approximating full-precision values. </li> <li>Moreover, we thoroughly review the training of a quantized DNN, including the use of a straight-through estimator and quantization regularization. We explain the replacement of floating-point operations with low-cost bitwise operations in a quantized DNN and the sensitivity of different layers in quantization. </li> <li>Furthermore, we highlight the evaluation metrics for quantization methods and important benchmarks in the image classification task. We also present the accuracy of the state-of-the-art methods on CIFAR-10 and ImageNet. </li> <li>This article attempts to make the readers familiar with the basic and advanced concepts of quantization, introduce important works in DNN quantization, and highlight challenges for future research in this field.</li> </ol> </li> </ul>"},{"location":"Paper/Quantization/1/#1","title":"1 \u7b80\u4ecb","text":"<ol> <li>Deep Convolutional Neural Network (DCNN) \u6210\u5c31\u6590\u7136\uff0c\u4f46\u9700\u8981\u5b58\u50a8\u5927\u91cf\u53c2\u6570\uff0c\u8fdb\u884c\u5927\u91cf\u8ba1\u7b97\uff08The main operation in DCNNs is multiply-accumulate (MAC) in convolution and Fully Connected (FC) layers.\uff09\uff0c\u6240\u4ee5DNNs\u7684\u52a0\u901f\u5f88\u6709\u5fc5\u8981\u3002</li> <li>In the beginning, the focus was on hardware optimization for processing speedup in DNN accelerators. -&gt;  Later, researchers concluded that compression and software optimization of DNNs can be more effective before touching hardware.</li> <li>The approaches in DNN compression:<ul> <li>Quantization: approximates the numerical network components with low bit-width precision.</li> <li>Pruning: removing unnecessary or less important connections within the network and making a sparse network that reduces memory usage as well as computations.</li> <li>Low-rank approximation: an approach to simplify matrices and images, creates a new matrix close to the weight matrix, which has lower dimensions and fewer computations in DNNs.</li> <li>Knowledge Distillation (KD): employ a simpler model that exhibits generalization and accuracy comparable to the complex model.</li> </ul> Advantages of quantization <ul> <li>High compression, with less accuracy reduction.</li> <li>Flexibility -- Since quantization is not dependent on the network architecture, a quantization algorithm can be applied to various types of DNNs. (Many quantization methods originally designed for DCNNs are also used for Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks) .</li> <li>Smaller number of cycles on hardware -- as high-cost floating-point operations are replaced with low-cost operations.</li> <li>Reduces the cost of hardware accelerator design. -- For instance, in 1-bit quantization, a 32-bit floating-point multiplier can be replaced an XNOR operator, leading to a cost reduction of 200 times in Xilinx FPGA</li> <li>Contribute to controlling overfitting. -- By simplifing parameters</li> </ul> </li> <li>\u5217\u4e3e\u4e86\u4e00\u4e9b\u5173\u4e8e\u5404\u79cdQuantization\u65b9\u5f0f\u7684\u53c2\u8003\u6587\u732e (binary,mixed-precision,distillation-assisted,hardware-aware......)</li> <li>\u6982\u51b5\u540e\u7eed\u5185\u5bb9 (\u540cabstract)</li> </ol>"},{"location":"Paper/Quantization/1/#2","title":"2 \u795e\u7ecf\u7f51\u7edc\u57fa\u672c\u6982\u5ff5","text":"<ol> <li>A DCNN consists of various types of layers, and the common layers include convolution layer, normalization layer, pooling layer, and FC layer.</li> <li> <ul> <li>The main layer in DCNN is the convolution layer, which is formed in three     dimensions. </li> <li>This layer produces an output feature map by convolving multiple filters (weights) with the input feature map. </li> <li>There is weight sharing in the convolution layer, which means that each weight is applied to different connections. </li> <li>The majority of computationsin DCNNs are in this layer due to its three-dimensional structure and weight sharing.</li> </ul> </li> <li>Weight sharing -&gt; a significant reduction in the number of parameters. -&gt; the majority of parameters in DCNNs are typically in the FC layers, where each neuron is connected to all neurons in both the previous and next layers.</li> <li>As the convolution and FC layers contain the majority of computations and parameters in DCNNs, the primary focus is on these layers in accelerators and compression techniques.</li> </ol>"},{"location":"Paper/Quantization/1/#3","title":"3 \u91cf\u5316\u7684\u6982\u5ff5","text":"<p>Quantization is mapping values from a continuous space to a discrete space, where full-precision values are mapped to new values with lower bit-width called quantization levels.</p>"},{"location":"Paper/Quantization/1/#31","title":"3.1 (\u7f51\u7edc\u7ec4\u6210\u4e2d)\u91cf\u5316\u7684\u5bf9\u8c61","text":"<ul> <li>Each numerical component in neural networks can be quantized. \u200cThese components are typically divided into three main categories: weights, activations, and gradients.</li> <li>Weights: the most common (But in most cases, biases and other parameters, such as batch normalization parameters, are kept in full precision in view of the fact that they include a minimal rate of neural network parameters, and the quantization of them is less efficient in compression.)</li> <li>Activations: \u6bd4weights\u7684\u91cf\u5316\u66f4\u56f0\u96be (While weightsremain fixed after training, activations change during the inference phase according to the input data.)\uff0c\u4f46\u4ec5\u4ec5\u5bf9weights\u91cf\u5316\u6548\u7387\u4e0d\u9ad8\u3001\u5185\u5b58\u4f7f\u7528\u7387\u4f4e, \u6545\u9700\u5171\u540c\u91cf\u5316</li> <li>Gradients: \u4ec5\u5bf9\u8bad\u7ec3\u65f6\u7684\u52a0\u901f\u6709\u7528\uff0c\u66f4\u96be\u91cf\u5316 (While weightsremain fixed after training, activations change during the inference phase according to the input data.)</li> </ul>"},{"location":"Paper/Quantization/1/#32","title":"3.2 \u4f55\u65f6\u91cf\u5316","text":"Quantization-Aware Training (QAT) Post-Training Quantization (PTQ). When During training After training Defect In a low bit-width precision quantized network, the convergence of the learning algorithm is challenging. -&gt; requires more iterations than the full-precision network for convergence. It needs customized solutions compatible with a discrete network. A reduction in model accuracy. -&gt; needs retraining, fine-tuning agter quantization -&gt; repeat to reach an acceptable accuracy. Speedup phase the training and inference phases the inference phase <ul> <li>The model accuracy in the QAT approach is commonly higher than in PTQ, because the trained model is more compatible with the quantization process.</li> </ul>"},{"location":"Paper/Quantization/1/#33-deterministicstochastic","title":"3.3 \u786e\u5b9a\u6027(deterministic)\u548c\u968f\u673a(stochastic)\u91cf\u5316","text":""},{"location":"Paper/Quantization/1/#deterministitc-quantization-methods","title":"Deterministitc Quantization Methods","text":"<ul> <li>Binary quantization:</li> </ul> \\[b=\\text{Sign}\\left(x\\right)=\\begin{cases}+1\\quad x\\geq0\\\\-1\\quad x&lt;0\\end{cases}.\\] <ul> <li>Ternary quantization:</li> </ul> \\[t=\\begin{cases}+1&amp;x&gt;\\Delta\\\\0&amp;|x|\\leq\\Delta\\\\-1&amp;x&lt;-\\Delta\\end{cases}.\\] <ul> <li>Others:</li> </ul> \\[Q\\left(x\\right)=Sign\\left(x\\right).d.\\min\\left(round\\left(\\frac{\\left|x\\right|}d\\right),\\frac{M-1}2\\right)\\] <p>d represents the step size, and M is an odd number and determines the number of quantization levels. Consequently, the quantization levels include zero, positive, and negative values symmetrically.</p> \\[quantize_k=\\frac1{2^k-1}round\\left(\\left(2^k-1\\right)x\\right),0\\leq x\\leq1\\] <p>It maps the full-precision values in the range \\(x\\in[0,1]\\) to \\(2^k\\) quantization levels within the same interval with step size \\(\\frac1{2^k-1}.\\) For \\(k\\) bit-width, the quantization levels are \\(L_q=\\) \\(\\{0,\\frac1{2^k-1},\\frac2{2^k-1},\\ldots,1\\}.\\) For example, for \\(k=2\\),there are \\(2^2=4\\) quantization levels, which are \\(L_q=\\) \\(\\{ 0\\), \u2153, \u2154, \\(1\\} .\\)</p> <ul> <li>Learned Quantization Network (LQ-Nets):</li> </ul> \\[Q\\left(x,v\\right)=v^Te_l\\ e_l\\in\\left\\{-1,1\\right\\}^K,x\\in\\left(t_l,t_{l+1}\\right).\\] <p>x represents full-precision values, \\(\\nu\\in\\mathbb{R}^{K}\\) denotes the learnable floating-point basis vector, and \\(e_{l}\\) is a \\(k\\)-bit binary vector from \\([-1,-1,\\ldots,-1]\\mathrm{~to~}[1,1,\\ldots,1].\\)</p>"},{"location":"Paper/Quantization/1/#stochastic-quantization-methods","title":"Stochastic Quantization Methods","text":"\\[b=\\begin{cases}+1\\:p=\\sigma\\:(x)\\\\-1\\:q=1-p\\end{cases}.\\] \\[\\sigma\\left(x\\right)=clip\\left(\\frac{x+1}2,0,1\\right)=\\max\\left(0,\\min\\left(1,\\frac{x+1}2\\right)\\right).\\] \\[\\begin{cases}\\mathrm{if~}w&gt;0:p\\left(t=1\\right)=w;\\quad p\\left(t=0\\right)=1-w\\\\\\mathrm{if~}w&lt;0:p\\left(t=-1\\right)=-w;p\\left(t=0\\right)=1+w\\end{cases}.\\]"},{"location":"Paper/Quantization/1/#deterministic-and-stochastic-quantization-comparison","title":"Deterministic and Stochastic Quantization Comparison.","text":"<ul> <li>Stochastic quantization has shown better model generalization compared to deterministic quantization.</li> <li>Implementation of stochastic quantization is more challenging and costly than deterministic quantization, particularly in hardware implementations, as it requires a random bit generator.</li> </ul>"},{"location":"Paper/Quantization/1/#34","title":"3.4 \u7531\u5206\u5e03\u770b\u91cf\u5316\u65b9\u6cd5\u7684\u6c34\u5e73","text":""},{"location":"Paper/Quantization/1/#341-uniform-and-non-uniform-quantization","title":"3.4.1 Uniform and Non-uniform Quantization","text":"<ul> <li>In non-uniform quantization, the step size is determined according to the distribution of the full-precision values, which makes it more complex and accurate than uniform quantization.</li> </ul> base-2 logarithm quantization: (x represents full-precision values) \\[Q\\left(x\\right)=\\mathrm{Sign}\\left(x\\right)2^{round\\left(\\log_2\\left|x\\right|\\right)}\\] <p>-&gt; Logarithmic quantization allows the encoding of a larger range of numbers using the same storage in comparison with uniform quantization by storing a small integer exponent instead of a floating-point number.</p> <ul> <li> <p>Previous studies have revealed that weights in DCNNs often follow a normal distribution with a mean of zero:   -&gt; In logarithmic quantization, the quantization levels are denser for values close to zero. Therefore, the distribution of quantization levelsin logarithmic quantization is matched to the distribution of the full-precision weights in DCNNs, which leads to more accurate quantization.</p> </li> <li> <p>The base-2 logarithm quantization is naturally a representation of the binary system. -&gt; it is well-matched to digital hardware and provides simple operations. </p> </li> </ul>"},{"location":"Paper/Quantization/1/#342-clustering-based-quantization","title":"3.4.2 Clustering-based Quantization.","text":"<ul> <li>DeepCompression method: using the k-means algorithm, where the weight values in a cluster are close to each other and mapped to the same quantization level, which is the cluster center.</li> <li>Single Level Quantization (SLQ) (for high bit-width precision ): the weights of each layer are clustered separately using the k-means algorithm. -&gt; the clusters are grouped into two categories based on quantization loss. -&gt; Low loss: quantization  ; High loss: retrain -&gt; These steps are repeated until all the weights are quantized. -&gt; SLQ is not suitable for low bit-width quantization due to the small number of clusters, which leads to significant quantization loss.</li> <li>Multiple Level Quantization (MLQ) (for 2-bit and 3-bit quantization.): -&gt; (Compared to SLQ) partitions weights not only in the width but also in the depth of the network. Layers are quantized iteratively and incrementally (not at the same time).</li> <li>Extended Single Level Quantization (ESLQ): changes cluster centers as quantization levels to values with a specific type. For example, quantization levels are mapped to the closest number in the form of Power Of Two (POT), making it well-suited for implementation on FPGA platforms.</li> </ul> Weighted entropy measure (for evaluating the quality of clustering) Challenges of the clustering-based approach <ol> <li>Not suitable for implementation in hardware and software due to their significant time complexity and computational requirements for codebook reconstruction.  </li> <li>The weights within a cluster are not contiguous in memory, which leads to irregular memory accesses with long delays.  </li> <li>The clustering-based approach is not suitable for activations quantization. (As weights are fixed during training, but activations are not)</li> </ol>"},{"location":"Paper/Quantization/1/#343-scale-factor","title":"3.4.3 Scale Factor.  (\u672a\u603b\u7ed3)","text":"<ul> <li>\u8bb2\u4e86 Ternary Weight Network (TWN) method, Accurate Binary Convolutional (ABC) method, Trained Ternary Quantization (TTQ), Explicit Loss-error-aware Quantization (ELQ) method......</li> <li>The effect of scale factor model on convergence &amp; Challenges of using scale factor</li> </ul>"},{"location":"Paper/Quantization/1/#4","title":"4 \u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3","text":"\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5e38\u7528\u7684\u8bad\u7ec3\u65b9\u5f0f--EBP (\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u3001\u94fe\u5f0f\u6cd5\u5219\u6765\u8c03\u6574\u7f51\u7edc\u53c2\u6570) <p>  The weights between layers \\(k\\) and \\(j\\) are updated as  </p> \\[w_{kj}\\left(n\\right)=w_{kj}\\left(n-1\\right)-\\gamma\\Delta w_{kj},\\:\\Delta w_{kj}=\\frac{\\partial E}{\\partial w_{kj}}=\\sum_{i\\in I_{j}}\\left(\\delta_{i}w_{ji}\\right)h'\\left(x_{i}\\right)y_{k}=\\delta_{j}y_{k}.\\] <p>In the Equation, \\(w_{kj}(n-1)\\) and \\(w_{kj}(n)\\) indicate the weights between \\(k\\) and \\(j\\) layers before and after the update, respectively. \\(\\gamma\\) and \\(\\delta\\) are the learning rate and the error signal, respectively. \\(x_i\\) and \\(\\gamma_i\\) are the inputs and outputs of layer \\(i\\), respectively, and \\(h^\\prime\\) denotes the derivative of the activation function.  </p> <ul> <li>\u7136\u800c\u4f7f\u7528\u8be5\u65b9\u6cd5\u9700\u8981\u201c\u8fde\u7eed\u3001\u7cbe\u786e\u3001\u4e0d\u542b\u9a7b\u70b9\u4e0e\u4e0d\u53ef\u5fae\u5206\u70b9\u201d\u7684\u51fd\u6570\uff0c\u800c\u8fd9\u5728\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f80\u5f80\u4e0d\u6ee1\u8db3\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e38\u4f7f\u7528STE\u65b9\u6cd5\u6765\u4f30\u8ba1\u4e0d\u53ef\u5fae\u51fd\u6570\u7684\u68af\u5ea6\u3002</li> </ul>"},{"location":"Paper/Quantization/1/#41-straight-through-estimator","title":"4.1 Straight-through Estimator","text":""},{"location":"Paper/Quantization/1/#411-ste-for-the-sign-function","title":"4.1.1 STE for the \\(Sign\\) function","text":"<p>Examples: - Bitwise Neural Networks -- (a) - XNOR-Net and Binarized Neural   Networks (BNN) -- (b) -- hard tanh (Clip) - Bi-RealNet -- (c&amp;d)  </p> <p>(The authors of the Bi-RealNet paper concluded that higher-order functions require more complex computations, and thus, the second-order function is acceptable)</p>"},{"location":"Paper/Quantization/1/#412-error-decay-estimator-ede","title":"4.1.2 Error Decay Estimator (EDE)","text":"<p>Information Retention Network (IR-Net) method  </p> \\[F\\left(x\\right)=k\\tanh\\left(tx\\right),\\] <p>where \\(k\\) and \\(t\\) are computed as</p> \\[t=T_{min}10^{\\frac{i}{N}\\times log\\frac{Tmax}{T_{min}}},\\:k=\\max\\left(\\frac{1}{t},1\\right).\\] <p>In the Equation, \\(i\\) determines the epoch number in \\(N\\) epochs, \\(T_{min}\\) is set to \\(10^{-1}\\),and \\(T_{max}\\) is set to 10.EDE lies between the identity function \\((y=x)\\) and the \\(hard \\  tanh\\) function. \\(Hard \\  tanh\\) is close to the \\(Sign\\) function, but it discards the parameters outside the range [-1,1].  </p> Note <p>Consequently, those parameters are not updated anymore, leading to a loss of information. However, the identity function covers the parameters outside [-1,1] but has a significant difference from the \\(Sign\\) function, as indicated by the shaded area in Figure 12. EDE makes a tradeoff between the identity and hard tanh functions by varying parameters \\(k\\) and \\(t\\) during training. Initially, \\(k\\) is bigger than 1,making EDE closer to the identity function. As the number of epochs increases, \\(k\\) gradually tends towards 1, causing EDE transition to hard tanh for achieving more accurate estimation.  </p>"},{"location":"Paper/Quantization/1/#413-quantized-relu-and-ste","title":"4.1.3 Quantized ReLU and STE","text":"Log-tailed ReLU function \\[\\tilde{Q}=\\begin{cases}q_m+\\log\\left(x-\\tau\\right)\\:x&gt;q_m\\\\x&amp;0&lt;x\\leq q_m\\:,\\quad\\tau=q_{m-1}\\\\0&amp;x\\leq0\\end{cases}\\] <ul> <li>The derivative of Half-Wave Gaussian Quantization (HWGQ) is zero.</li> <li>HWGQ is bounded to qm for x&gt;0, whereas Vanilla ReLU tends to infinity. Consequently, using Vanilla ReLU in the backward pass leads to inaccurate gradients and unstable learning during training.</li> <li>In Clipped ReLU, the weak point of the Vanilla ReLU is modified by setting the gradient to zero for x\u2265qm. The idea behind this modification comes from the fact that the frequency of the large values is commonly low, and these values are interpreted as outliers.</li> <li>Experimental results in the HWGQ method  show that Log-tailed ReLU achieves higher accuracy in AlexNet compared to Clipped ReLU. However, Clipped ReLU achieves superior performance compared to Log-tailed ReLU in VGGNet-variant and ResNet-18, which are deeper than AlexNet.  </li> </ul>"},{"location":"Paper/Quantization/1/#414-bounded-rectifier-and-ste","title":"4.1.4 Bounded rectifier and STE","text":"Note <p>In the ABC method , a bounded rectifier activation function was proposed for mapping the full-precision activations to the range [0,1] as</p> \\[h_v\\left(x\\right)=clip\\left(x+v,0,1\\right)=\\begin{cases}1&amp;x+v&gt;1\\\\x+v&amp;0&lt;x+v&lt;1\\\\0&amp;x+v&lt;0\\end{cases}.\\] <p>In Equation represents a trainable shift parameter. After mapping the values to the range [0,1],Equation is utilized for binarization as</p> \\[A=H_\\upsilon\\left(R\\right)=2\\mathbb{I}_{h_\\upsilon\\left(R\\right)\\geq0.5}-1=\\begin{cases}+1\\:h_\\upsilon\\geq0.5\\\\-1\\:h_\\upsilon&lt;0.5\\end{cases}.\\] <p>In Equation, l indicates the Indicator function. In the backward pass, STE is applied, and the gradient is computed as</p> \\[\\frac{\\partial c}{\\partial R}=\\frac{\\partial c}{\\partial A}\\mathbb{I}_{0\\leq R-v\\leq1}.\\]"},{"location":"Paper/Quantization/1/#415-parametric-quantization-function-and-ste","title":"4.1.5 Parametric quantization function and STE","text":"Note \\[y=PACT\\left(x\\right)=0.5\\left(\\left|x\\right|-\\left|x-\\alpha\\right|+\\alpha\\right)=\\begin{cases}0\\:x\\in\\left(-\\infty,0\\right)\\\\x\\:x\\in\\left[0,\\alpha\\right)\\\\a\\:x\\in\\left[\\alpha,+\\infty\\right)\\end{cases}.\\] <p>(45)</p> <p>The PACT function maps the full-precision activations to the range [0, \\(\\alpha].\\) Then the output of the PACT function is quantized to \\(k\\) bit-width precision using Equation (46).</p> \\[y_q=round\\left(y\\frac{2^k-1}\\alpha\\right)\\frac\\alpha{2^k-1}\\] <p>(46)</p> <p>If \\(\\alpha\\) is equal to 1,then the PACT function corresponds to the bounded rectifier function with \\(\\upsilon=0\\) in the ABC method [117]. The optimum \\(\\alpha\\) is found during training for minimizing the accuracy drop in quantization. It should be noted that the optimum value varies across different layers and models. Since Equation (46) is not differentiable, STE is employed for updating \\(\\alpha:\\)</p> \\[\\frac{\\partial y_q}{\\partial\\alpha}=\\frac{\\partial y_q}{\\partial y}\\frac{\\partial y}{\\partial\\alpha}=\\begin{cases}0&amp;x\\in(-\\infty,\\alpha)\\\\1&amp;x\\in[\\alpha,+\\infty)\\end{cases}.\\] <p>(47)</p> <p>The training is dependent on the value of \\(\\alpha.\\)If the initial value of \\(\\alpha\\) is too small, then, according to Equation (47), most activations will fall in the range of non-zero gradient, causing frequent changes in the value of \\(\\alpha\\) during training and leading to low model accuracy. However, if the initial value of \\(\\alpha\\) is too large, then the gradient will be zero for the majority of activations, leading to small gradients and the risk of gradient vanishing in the EBP algorithm. To address this, \\(\\alpha\\) is initialized with a large value that is not excessively large and then reduced using L2-norm regularization. Although the effectiveness of STE has been demonstrated in practice through the results of previous works, there is still a concern regarding the lack of theoretical proof for its performance. Therefore, in recent years, some researchers have made efforts to theoretically justify the performance of STE [138, 139] .  Table 3 summarizes the forward quantization function and its estimator in the backward pass using STE for several previous works.</p>"},{"location":"Paper/Quantization/1/#42","title":"4.2 \u6743\u503c\u66f4\u65b0","text":"<ul> <li>\u4e3a\u9632\u6b62\u65e0\u6548update\uff0c\u5e38\u5e38\u5728update weights\u8fc7\u7a0b\u4e2d\u4f7f\u7528full-precision.</li> </ul>"},{"location":"Paper/Quantization/1/#43","title":"4.3 \u8bad\u7ec3\u53c2\u6570","text":"<ul> <li>In quantized networks, a smalel learning rate often yields better.</li> </ul>"},{"location":"Paper/Quantization/1/#432-network-structure","title":"4.3.2 Network Structure:","text":"<ul> <li>Sometimes structural adjustments are necessary for the neural network after quantization.</li> </ul> example <p>For instance, the max-pooling layer in some binary DNNs is displaced. In DCNNs, a max-pooling layer commonly comes immediately after the activation layer. However, in a binary neural network, where the Sign function is used for the binarization of the activations, placing the max-pooling layer immediately after the Sign function results in an output matrix containing only +1 elements, as the values in a binarized matrix are \u22121 and +1. This leads to a loss of information.</p>"},{"location":"Paper/Quantization/1/#433-regelarization","title":"4.3.3 Regelarization:","text":"<ul> <li>In quantization, approximating weights with low bit-width precision acts as a regularizer, and pushing the weights toward zeros can lead to a significant drop in accuracy.  </li> </ul> some approaches <ul> <li>Bit-level Sparsity Quantization (BSQ) suggested a regularization for mixed-precision quantization.  </li> <li>a periodic regularization to push the full-precision weights toward the quantization levels.  </li> <li>Tang et al. introduced a new regularization for binary quantization:</li> </ul> \\[ J\\left(W,b\\right)=L\\left(W,b\\right)+\\lambda\\sum_{l=1}^L\\sum_{i=1}^{N_l}\\sum_{j=1}^{M_l}\\left(1-\\left(W_{l,ij}\\right)^2\\right)\\\\J\\left(W,b\\right)=L\\left(W,b\\right)+\\lambda\\sum_{l=1}^L\\sum_{i=1}^{N_l}\\sum_{j=1}^{M_l}\\left(1-\\left(W_{l,ij}\\right)^2\\right)\\] <p>In Equation, \\(L(W,b)\\) represents the loss function, and the second term denotes the regularization relation. \\(L\\) indicates the number of layers. \\(N_l\\) and \\(M_l\\) are the dimensions of the weight matrix in layer \\(\\iota\\) The parameter \\(\\lambda\\) controls the effect of the loss function and regularization term.  -   </p>"},{"location":"Paper/Quantization/1/#5","title":"5 \u91cf\u5316\u7f51\u7edc\u4e2d\u7684\u64cd\u4f5c","text":"<p>\u8ba8\u8bba\u4e86\u5728\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08DCNNs\uff09\u4e2d\u5b9e\u73b0\u4e58\u7d2f\u52a0\uff08MAC\uff09\u64cd\u4f5c\u7684\u6548\u7387\u95ee\u9898\u3002MAC\u64cd\u4f5c\u662fDCNNs\u4e2d\u7684\u4e3b\u8981\u8ba1\u7b97\u4efb\u52a1\uff0c\u901a\u5e38\u6d89\u53ca\u5927\u91cf\u7684\u6d6e\u70b9\u6570\u4e58\u6cd5\u548c\u52a0\u6cd5\u3002\u91cf\u5316\u6280\u672f\u901a\u8fc7\u5c0632\u4f4d\u6d6e\u70b9\u6570\u6620\u5c04\u5230\u66f4\u4f4e\u6bd4\u7279\u5bbd\u5ea6\uff08\u59828\u4f4d\u30014\u4f4d\u30012\u4f4d\u548c1\u4f4d\uff09\u7684\u503c\uff0c\u4ece\u800c\u5728\u786c\u4ef6\u5e73\u53f0\u4e0a\u7528\u66f4\u9ad8\u6548\u7684\u6574\u6570\u6216\u4f4d\u64cd\u4f5c\u66ff\u6362\u9ad8\u6210\u672c\u7684\u6d6e\u70b9\u64cd\u4f5c\u3002  </p> <ul> <li>\u91cf\u5316\u64cd\u4f5c\uff1a\u4ecb\u7ecd\u4e86\u51e0\u79cd\u91cf\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9\u6570\u91cf\u5316\u548cPOT\uff08Power Of Two\uff09\u91cf\u5316\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u4f4d\u79fb\u52a8\u64cd\u4f5c\u6765\u66ff\u4ee3\u6d6e\u70b9\u4e58\u6cd5\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002  </li> <li>\u4f4d\u64cd\u4f5c\uff1a\u8ba8\u8bba\u4e86\u5982\u4f55\u5728\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u4e2d\u4f7f\u7528\u4f4d\u64cd\u4f5c\uff08\u5982XNOR\u548cAND\uff09\u6765\u6267\u884cMAC\u64cd\u4f5c\uff0c\u8fd9\u5728\u786c\u4ef6\u5b9e\u73b0\u4e2d\u975e\u5e38\u6709\u6548\u7387\u3002  </li> <li>\u96f6\u6c34\u5e73\u91cf\u5316\uff1a\u63a2\u8ba8\u4e86\u5728\u91cf\u5316\u4e2d\u5b9a\u4e49\u96f6\u6c34\u5e73\u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u56e0\u4e3a\u4e58\u4ee5\u96f6\u7684\u64cd\u4f5c\u53ef\u4ee5\u88ab\u7701\u7565\u3002  </li> </ul>"},{"location":"Paper/Quantization/1/#6","title":"6 \u91cf\u5316\u7f51\u7edc\u4e2d\u7684\u5c42","text":"<p>\u5206\u6790\u4e86\u5728DCNNs\u4e2d\u91cf\u5316\u4e0d\u540c\u5c42\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002\u7f51\u7edc\u4e2d\u7684\u6bcf\u5c42\u5bf9\u91cf\u5316\u7684\u654f\u611f\u6027\u4e0d\u540c\uff0c\u8fd9\u53d6\u51b3\u4e8e\u5c42\u7684\u7ed3\u6784\u548c\u5728\u7f51\u7edc\u4e2d\u7684\u4f4d\u7f6e\u3002</p> <ul> <li>\u7b2c\u4e00\u5c42\u548c\u6700\u540e\u4e00\u5c42\uff1a\u8ba8\u8bba\u4e86\u91cf\u5316\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u8fd9\u4e9b\u5c42\u5bf9\u91cf\u5316\u7279\u522b\u654f\u611f\uff0c\u56e0\u4e3a\u5b83\u4eec\u76f4\u63a5\u5173\u7cfb\u5230\u7f51\u7edc\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002  </li> <li>\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff1a\u4ecb\u7ecd\u4e86\u6839\u636e\u7f51\u7edc\u5c42\u7684\u654f\u611f\u6027\u4e3a\u6bcf\u5c42\u5206\u914d\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6\u7684\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u5728\u51c6\u786e\u6027\u548c\u538b\u7f29\u7387\u4e4b\u95f4\u627e\u5230\u6700\u4f73\u6743\u8861\u3002  </li> <li>\u5c42\u7684\u91cd\u8981\u6027\uff1a\u8ba8\u8bba\u4e86\u5982\u4f55\u8bc4\u4f30\u6bcf\u5c42\u5728\u91cf\u5316\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\u6765\u51b3\u5b9a\u6bcf\u5c42\u7684\u91cf\u5316\u7cbe\u5ea6\u3002  </li> </ul>"},{"location":"Paper/Quantization/1/#7","title":"7 \u8bc4\u4f30\u4e0e\u8ba8\u8bba","text":"<p>\uff08\u542b\u5927\u91cf\u6570\u636e\u3001\u8868\u683c\uff0c\u8ba8\u8bba\u5bf9\u6bd4\u4e86\u5404\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff09</p>"},{"location":"Paper/Quantization/1/#8","title":"8 \u603b\u7ed3\u4e0e\u5c55\u671b","text":"Abstract <ul> <li>In this article, we surveyed the previous quantization works in the image classification task. The basic and advanced concepts of DCNN quantization were discussed, as well as the most important methods and approaches in this field, along with their advantages and challenges. Some previous works perform quantization on both weights and activations, offering a higher compression rate and employing lower-cost operations compared to approaches that are quantized only weights. However, quantization of activations is more challenging compared to weights, which is due to the wide range of activations, the use of a non-differentiable activation function, the estimation of activations during the backward pass, and the variation of activation values during inference.  </li> <li>The QAT and PTQ methods were studied, and it is concluded that the QAT methods generally achieve higher accuracy than the PTQ methods in the inference phase. Training a quantized DNN poses new challenges compared to a full-precision network since the units are discrete. Itcommonly requires additional iterations for convergence in contrast to training a full-precision network, and adaptive training strategies are required to build an accurate model. For instance, the adjustment of learning rate and regularization techniques can be different from the training in the full-precision network.  </li> <li>We discussed uniform and non-uniform quantization techniques and concluded that nonuniform quantization, especially the POT quantization approach, efficiently covers the distribution of full-precision values, which leads to enhanced accuracy. For decreasing quantization error, it is important to allocate quantization levels to informative regions. Using the scale factor helps in shifting the quantization levels to the most informative parts of data.  </li> <li>Some previous methods have successfully achieved high accuracy on large-scale datasets, such as ImageNet, when both weights and activations are quantized in low bit-width. However, quantization with a precision lower than 4 bits remains a challenging task, especially in deeper networks. During the training of a quantized network, STE is commonly used for calculating gradients in the backward pass. The noise resulting from gradient mismatch, due to inaccurate estimation, is amplified layer-by-layer from the end of the network to the initial layers. This amplification of noise is more considerable in deeper networks compared to shallow networks. In the training, this noise can have a negative impact on model convergence. Additionally, since the number of parameters increases with the depth of the neural network, the range of parameters in the deeper networks is wider than in shallow ones, and the quantization is more challenging. Accordingly, future works should focus on addressing the quantization of weights and activations in deeper networks with low bit-width, such as binary or ternary quantization.  </li> <li>In this article, we discussed mixed-precision, which is currently an interesting approach in the quantization of the DNNs. The main challenge in mixed-precision quantization is the exponential time complexity in finding the optimum bit-width for each layer. It is desirable for future worksto develop solutionsthat can determine the optimum mixed-precision with polynomial time complexity.</li> </ul>"},{"location":"Paper/Quantization/2/","title":"(Survey) Model Quantization and Hardware Acceleration for Vision Transformers","text":""},{"location":"Paper/Quantization/2/#_1","title":"(Survey) Model Quantization and Hardware Acceleration for Vision Transformers","text":""},{"location":"Paper/Quantization/2/#paper-model-quantization-and-hardware-acceleration-for-vision-transformers-a-comprehensive-survey","title":"Paper: Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey","text":"<p>Abstract</p> <ul> <li>\u80cc\u666f\uff1aVision Transformers (ViTs) \u5728\u8bb8\u591a\u89c6\u89c9\u9886\u57df\u6b63\u66ff\u4ee3convolutional neural networks (CNNs) \uff0c\u4f46ViTs\u7684\u6a21\u578b\u5927\u3001\u7b97\u529b\u3001\u5b58\u50a8\u9700\u6c42\u5927\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\uff0c\u8fd9\u7a81\u51fa\u4e86 algorithm-hardware co-design \u7684\u91cd\u8981\u6027\u3002\u800c\u91cf\u5316\u6a21\u578b\uff08\u7b97\u6cd5\uff09\u4fbf\u53ef\u5f88\u597d\u5730\u63d0\u9ad8\u5176\u6548\u7387\u3002</li> <li>\u8d21\u732e\uff08survey\uff09\uff1a  <ol> <li>\u4e00\u4e9b\u72ec\u7279\u7684 ViTs \u67b6\u6784\u7684\u7279\u6027\u4e0e\u8fd0\u884c\u65f6\u95f4\u3002  </li> <li>\u6a21\u578b\u91cf\u5316\u7684\u57fa\u672c\u539f\u7406\uff0c\u5bf9\u6700\u5148\u8fdb\u7684 ViTs \u91cf\u5316\u6280\u672f\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002</li> <li>\u91cf\u5316 ViTs \u7684\u786c\u4ef6\u52a0\u901f\uff0c\u7a81\u51fa\u4e86 hardware-friendly algorithm \u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002</li> <li>\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u5c55\u671b</li> </ol> </li> </ul>"},{"location":"Paper/Quantization/2/#1","title":"1 \u7b80\u4ecb","text":"<p>\u80cc\u666f + \u76ee\u524d\u90e8\u5206 survey \u7684\u95ee\u9898\uff08\u5ffd\u7565 hardware \u6216 algorithm\uff0c\u4e3b\u8981\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\uff09 + \u672c\u6587\u7ed3\u6784\uff08\u76ee\u5f55\uff09</p>"},{"location":"Paper/Quantization/2/#2-vits","title":"2 ViTs \u67b6\u6784\u53ca\u6027\u80fd\u5206\u6790","text":"<p>The Vision Transformers (ViTs), utilizing the self-attention mechanism to grasp \u201clong-range\u201d relationships in image sequences, \u5728 CV \u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u6210\u529f.  </p>"},{"location":"Paper/Quantization/2/#21-vits","title":"2.1 ViTs \u67b6\u6784\u6982\u8ff0","text":"details <ul> <li> <p>The process culminates with a fully-connected layer, termed the \u201cMLP Head\u201d, for classification purposes.  </p> </li> <li> <p>The MHA (Multi-Head Attention) module first projects the image sequence by multiplying it through distinct weight matrices \\(W^Q,W^K\\) and \\(W^V\\), generating query \\((Q)\\), key \\((K)\\),and value \\((V)\\) activation. The self-attention mechanism is then applied as follows:</p> </li> </ul> \\[\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\] <p>where \\(d_k\\) represents the dimensionality of the key vectors. The MHA aggregates information from multiple representation subspaces, synthesizing the outputs from different heads into a unified representation:</p> \\[\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h)W^O,\\quad\\] <p>with each head defined as:</p> \\[\\mathrm{head}_i=\\mathrm{Attention}(QW_i^Q,KW_i^K,VW_i^V).\\] <ul> <li>The FFN (Feed-Forward) module, which includes two dense layers activated by GELU , processes each token individually, augmenting the model's capacity to understand intricate functions:</li> </ul> \\[\\mathrm{FFN}(x)=GELU(xW_1+b_1)W_2+b_2.\\] <ul> <li>In summary, the MHA encompasses six linear operations, including four weight-to-activation transformations (\\(W^Q,W^K,W^V\\), and \\(W^O\\)) projections and two activationto-activation transformations \\(Q\\times K^T\\) and Out\\(_\\mathrm{softmax}\\times V.\\) In contrast, the FFN comprises two linear projections \\(W_1\\) and \\(W_2\\). Non-linear operations like Softmax, LayerNorm, and GELU, though less prevalent, present computational challenges on conventional hardware due to their complexity, potentially restricting the enhancement of end-to-end transformer inference.</li> </ul>"},{"location":"Paper/Quantization/2/#22-vits","title":"2.2 ViTs \u4e3e\u4f8b","text":"<ul> <li>ViTs \u662f\u8bb8\u591a\u56fe\u50cf\u5904\u7406\u9886\u57df\u7684 Transformer-based models \u7684\u57fa\u7840\u67b6\u6784\uff08\u5728\u5904\u7406\u4e0d\u540c\u89c4\u6a21\u53ca\u590d\u6742\u5ea6\u7684\u56fe\u50cf\u65f6 robustness \u826f\u597d\uff09\u3002</li> <li>DeiT \u5728 ViTs \u57fa\u7840\u4e0a \u201cBy incorporating a novel teacher-student strategy tailored for Transformers, which includes the use of distillation tokens, DeiT can effectively train on smaller datasets without a substantial loss in performance, demonstrating the model\u2019s adaptability to data constraints.\u201d  </li> <li>Swin-Transformer  integrating a hierarchical structure with shifted windows, improves the model\u2019s ability to capture local features while still maintaining a global context.  </li> </ul> More Vits <ul> <li>S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, \u201cTransformers in vision: A survey,\u201d ACM computing surveys (CSUR), vol. 54, no. 10s, pp. 1\u201341, 2022.  </li> <li>K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu, et al., \u201cA survey on vision transformer,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 87\u2013110, 2022.</li> </ul>"},{"location":"Paper/Quantization/2/#23-roofline-model-vits","title":"2.3 \u7528 Roofline Model \u5206\u6790 ViTs \u7684\u6027\u80fd","text":"<ol> <li>This model aids in identifying whether a layer or operation is a computation or memory bottleneck, thereby enabling optimal utilization of memory access and processing capabilities\u3002  </li> <li>In alignment with the roofline model, we evaluate the model\u2019s computational demand by measuring both the floating-point operations (FLOPs) and the memory operations (MOPs) involved. Following this, we determine the arithmetic intensity, calculated as the ratio of FLOPs to bytes accessed (FLOPs / B)(\u5373 Arithmetic Intensity = #FLOPs / #MOPs).  </li> <li>\u901a\u8fc7\u4e0d\u540c\u5927\u5c0f\u3001\u5c5e\u6027\u7684\u6d4b\u8bd5\u96c6\u5728\u4e0d\u540c\u5927\u5c0f ViTs \u4e0a\u7684\u6d4b\u8bd5\u4e0e\u5bf9\u6bd4\uff0c\u5f97\u51fa\uff1a \u9700\u8981\u5feb\u901f\u8bbf\u95ee\u5185\u5b58\u65f6 \u201cEmploying quantization to represent data in 8-bit or lower-bit formats can be especially advantageous, as it reduces the model\u2019s memory footprint, accelerates data transfer, thereby enhancing model inference.\u201d  \u5728\u8ba1\u7b97\u91cf\u53d7\u9650\u65f6 \u201cAdopting INT8 precision emerges as a crucial optimization in such compute-bound situations. This approach not only mitigates computational bottlenecks but also capitalizes on the enhanced efficiency and throughput of quantized computing, significantly boosting overall performance.\u201d \uff08\u7a81\u51fa\u4e86 Quantization \u7684\u91cd\u8981\u6027\uff09</li> </ol>"},{"location":"Paper/Quantization/2/#3-quantization","title":"3 \u91cf\u5316\uff08Quantization\uff09\u57fa\u7840","text":""},{"location":"Paper/Quantization/2/#31-linear-quantization","title":"3.1 Linear Quantization","text":"<ul> <li>Linear Quantization linearly maps the continuous range of weight or activation values to a discrete set of levels.  </li> </ul> \\[q=round(\\frac rS)+Z\\] <ul> <li>r represents input real numbers, q represents output quantized integers, S is a real-valued scaling factor, and Z is an integer zero point.</li> </ul> \\[\\tilde{r}=S(q-Z)\\] <ul> <li>\u7531\u4e8e rounding \u64cd\u4f5c\u7684\u8fd1\u4f3c\uff0c\u8be5\u65b9\u5f0f\u7684 dequantization \u5b58\u5728\u4e00\u5b9a\u8bef\u5dee\u3002 </li> </ul>"},{"location":"Paper/Quantization/2/#32-symmetric-and-asymmetric-quantization","title":"3.2 Symmetric and Asymmetric Quantization","text":"\\[S=\\frac{r_{max}-r_{min}}{q_{max}-q_{min}}\\] <p>where \\([r_{min},r_{max}]\\) and \\([q_{min},q_{max}]\\) represent the clipping range of real and integer values, respectively.</p> <ul> <li>The symmetric quantization \u5c06\u65b9\u7a0b\u4e2d\u7684\u5206\u5b50\u6362\u4e3a\u7edd\u5bf9\u503c \\(r_{max}-r_{min}=2\\max(|r_{max}|,|r_{min}|)= 2\\max(|r|).\\) </li> <li>If the clipping range is symmetric, the value of zero point Z becomes 0. Eq. \u53ef\u5316\u7b80\u4e3a \\(q=\\) \\(round(r/S)\\), \u66f4\u7cbe\u7b80\u9ad8\u6548.  </li> <li>As for the \\(q_{min}\\) and \\(q_{max}\\), we can choose to use the \u201cfull range\u201d or \u201crestricted range\u201d . In \"full range\" mode, \\(S=\\frac{2max(|r|)}{2^n-1}\\) . In \"restricted range\" mode \\(S=\\frac{max(|r|)}{2^{n-1}-1}\\) . In INT8, is [-128,127] and [-127,127] respectively. </li> <li>However, directly using the min/max values to determine the clipping range may be sensitive to outliers, resulting low resolution of quantization. Percentile and KL divergence  strategies are introduced to address this problem.  </li> </ul>"},{"location":"Paper/Quantization/2/#33-static-and-dynamic-quantization","title":"3.3 Static and Dynamic Quantization","text":"<ul> <li>\u52a8\u6001\u91cf\u5316\u5728\u8fd0\u884c\u65f6\u671f\u95f4\u4e3a\u6bcf\u4e2a activation map \u52a8\u6001\u786e\u5b9a\u8303\u56f4\uff0c\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u5373\u65f6\u8ba1\u7b97\u8f93\u5165\u5ea6\u91cf\uff08\u5982\u6700\u5c0f\u503c\u3001\u6700\u5927\u503c\u3001\u767e\u5206\u4f4d\u6570\u7b49\uff09\uff0c\u8fd9\u4f1a\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002\u4f46\u52a8\u6001\u91cf\u5316\u901a\u5e38\u53ef\u4ee5\u8fbe\u5230\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u7cbe\u786e\u5730\u786e\u5b9a\u6bcf\u4e2a\u5355\u72ec\u8f93\u5165\u7684\u4fe1\u53f7\u8303\u56f4\u3002  </li> <li>\u9759\u6001\u91cf\u5316\u53ef\u4ee5\u5728\u8fd0\u884c\u524d\u9884\u5148\u8ba1\u7b97\u548c\u786e\u5b9a\u8303\u56f4\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5b83\u901a\u5e38\u63d0\u4f9b\u7684\u51c6\u786e\u6027\u8f83\u4f4e\u3002  </li> <li>A common technique for pre-calculating the range is to process a set of calibration inputs to determine the average range of activations [35]. Mean Square Error (MSE) [38] and entropy [39] are often used as metrics to choose the best range. Additionally, the clipping range can also be learned during the training process [40].</li> </ul>"},{"location":"Paper/Quantization/2/#34-quantization-granularity","title":"3.4 Quantization Granularity","text":"<ul> <li>The determination of the clipping range can be categorized into layerwise, channelwise(common), and groupwise quantization, based on the level of granularity employed.  </li> <li>Accuracy (but also) overhead \u4f9d\u6b21\u9012\u589e.</li> </ul>"},{"location":"Paper/Quantization/2/#3536-post-training-quantization-ptq-and-quantization-aware-training","title":"3.5&amp;3.6 Post-Training Quantization (PTQ) and Quantization Aware Training","text":"<p>skip</p>"},{"location":"Paper/Quantization/2/#37-data-free-quantizationdfq-or-zsq","title":"3.7 Data Free Quantization(DFQ or ZSQ)","text":"<ul> <li>Data-Free Quantization (DFQ), alternatively termed ZeroShot Quantization (ZSQ), operates quantization independently of actual data.  </li> <li>\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u4f3c\u7684\u865a\u62df\u6570\u636e\uff0cthe synthetic data thus generated is then utilized to calibrate the model during PTQ and to perform fine-tuning within QAT.   </li> <li>\u53ef\u89e3\u51b3\u6570\u636e\u201c\u5927\u91cf\u3001\u9690\u79c1\u3001\u5b89\u5168\u201d\u95ee\u9898\u3002</li> </ul>"},{"location":"Paper/Quantization/2/#4-model-quantization-for-vits","title":"4 Model Quantization for ViTs","text":"<p>To do \uff08\u540e\u9762\u7684\u5185\u5bb9\u6709\u4e00\u70b9\u201c\u4ed9\u4e4b\u4eba\u516e\u5217\u5982\u9ebb\u201d\uff0c\u6211\u6682\u65f6\u8fd8\u6ca1\u6709\u80fd\u529b\u603b\u7ed3\uff09</p>"},{"location":"Paper/Quantization/2/#5-vits","title":"5 \u91cf\u5316 ViTs \u7684\u786c\u4ef6\u52a0\u901f","text":""},{"location":"Paper/Quantization/2/#6","title":"6 \u603b\u7ed3\u4e0e\u5c55\u671b","text":""},{"location":"Tools/mkdocs/commands/","title":"\u57fa\u7840\u547d\u4ee4","text":"<p>Abstract</p> <p>\u8bb0\u5f55\u4e00\u4e9b\u4f7f\u7528mkdocs\u7684\u57fa\u672c\u547d\u4ee4</p>"},{"location":"Tools/mkdocs/commands/#git","title":"git \u72b6\u6001\u67e5\u770b","text":"<pre><code>git status                  # \u67e5\u770b\u76f8\u5173\u6587\u4ef6\u72b6\u6001\ngit log                     # \u67e5\u770bgit\u65e5\u5fd7(\u54c8\u5e0c\u503c\u3001\u4f5c\u8005\u3001\u65f6\u95f4\u3001\u6ce8\u91ca)\ngit log --pretty=oneline    # \u67e5\u770bgit\u65e5\u5fd7(\u4ec5\u54c8\u5e0c\u503c\u3001\u6ce8\u91ca)\n</code></pre>"},{"location":"Tools/mkdocs/commands/#git_1","title":"git \u6587\u4ef6\u64cd\u4f5c","text":"<pre><code>git add .               # \u5c06\u6240\u6709\u5df2\u66f4\u6539\u6587\u4ef6\u6dfb\u52a0\u5230\u7f13\u5b58\u533a\ngit commit -m \"test\"    # \u5c06\u7f13\u51b2\u533a\u6587\u4ef6\u63d0\u4ea4\uff0c\u83b7\u5f97\u6700\u65b0\u7248\u672c\ngit commit -am \"test\"   # \u4e0a\u8ff0\u4e24\u6b65\uff0c\u4e00\u6b21\u89e3\u51b3 (\u65b0\u52a0\u5165\u7684\u6587\u4ef6\u9700\u8981\u521d\u6b21 add)\n\n# \u53ea\u80fd\u4fee\u6539\u5df2\u7ecf\u8ffd\u8e2a\u7684\u6587\u4ef6\u548c\u6587\u4ef6\u5939\n# \u4fee\u6539\u4e4b\u540e\uff0c\u76f8\u5f53\u4e8e\u6267\u884c\u4e86 add \uff0c\u76f4\u63a5 commit \u5c31\u53ef\u4ee5\u63d0\u4ea4\n\ngit mv -v oldfile newfile         # \u91cd\u547d\u540d\u6587\u4ef6(-v \u663e\u793a\u4fe1\u606f)\ngit mv -v oldfolder newfolder     # \u91cd\u547d\u540d\u6587\u4ef6\u5939(\u4e24\u79cd\u5747\u53ef)\ngit mv -v oldfolder/ newfolder/   # (\u82e5 newfolder \u6587\u4ef6\u5939\u539f\u672c\u5df2\u7ecf\u5b58\u5728\uff0c\u5219\u4f1a\u5c06 oldfolder \u79fb\u5165 newfolder)\n\nrm 'test.txt'           # \u5de5\u4f5c\u533a\u5220\u9664\u6587\u4ef6\ngit add test.txt        # \u7248\u672c\u5e93\u5220\u9664\u6587\u4ef6\uff0c\u8fd8\u9700\u8981 commit\ngit commit -m \"delete test\"\n</code></pre>"},{"location":"Tools/mkdocs/commands/#mkdocs","title":"mkdocs \u4f7f\u7528","text":"<pre><code># \u5728\u672c\u5730\u6e32\u67d3\u76f8\u5173\u7b14\u8bb0\uff0c\u53ef\u8bbe\u7f6e\u7aef\u53e3(\u59828000\uff0c\u5219\u7f51\u7ad9\u6839\u76ee\u5f55\u4e3a127.0.0.1:8000)\n\nmkdocs serve    \n\n# \u5c06\u672c\u5730\u7684\u4ed3\u5e93\u63d0\u4ea4\u5230Github\u4ed3\u5e93\u4e2d\u7684gh-pages\u5206\u652f(\u672c\u5730\u7684git\u5e94\u4e3aHEAD -&gt; main)\n\nmkdocs gh-deploy    \n\n# \u5c06Github\u4ed3\u5e93\u4e2d\u7684main\u5206\u652f\u66f4\u65b0\u81f3\u6700\u65b0(HEAD -&gt; main\u5904)\uff0c\u540c\u65f6\u5728\u7ebf\u7b14\u8bb0\u5c06\u66f4\u65b0\n\ngit push -u origin main \n</code></pre>"},{"location":"Tools/mkdocs/others/","title":"\u6742\u9879","text":"<p>Abstract</p> <p>\u8bb0\u5f55\u4e00\u4e9b\u4f7f\u7528mkdocs\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u89e3\u51b3\u65b9\u6848</p> \u5982\u4f55\u5728mkdocs\u4e2d\u4f7f\u7528\u6709\u8272\u7684\u6570\u5b66\u516c\u5f0f(\u5b57\u4f53\u4e0e\u80cc\u666f) \u4f7f\u7528 MathJax \u547d\u4ee4 \\color \\bbox <p>MathJax \u672c\u8eab\u652f\u6301\u4f7f\u7528 \\color \u547d\u4ee4\u66f4\u6539\u516c\u5f0f\u7684\u5b57\u4f53\u989c\u8272\uff0c\u7528 \\bbox \u547d\u4ee4\u66f4\u6539\u516c\u5f0f\u7684\u80cc\u666f\u989c\u8272\uff0c\u4f46\u5728 MkDocs \u4e2d\u4f60\u9700\u8981\u786e\u4fdd\u6b63\u786e\u914d\u7f6e MathJax: 1.\u5728 mkdocs.yml \u6587\u4ef6\u4e2d\uff0c\u542f\u7528 MathJax \u5e76\u81ea\u5b9a\u4e49\u914d\u7f6e\uff1a </p>YAML<pre><code>markdown_extensions:\n- pymdownx.superfences\n- pymdownx.highlight\n- pymdownx.arithmatex\n  - generic: true\nextra_javascript:\n- _js/mathjax.js  #\u914d\u7f6e\u65b9\u6cd5\u89c1https://squidfunk.github.io/mkdocs-material/reference/math/\n- https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js\n- https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n- https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\n- http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\n</code></pre> 2.\u5728 Markdown \u6587\u4ef6\u4e2d\u4f7f\u7528 \\color \u548c \\bbox \u547d\u4ee4\u6765\u8bbe\u7f6e\u989c\u8272: Markdown<pre><code>$$ \\color{red}{\\alpha + \\beta = \\gamma} $$\n$$ \\bbox[yellow]{\\color{red}{\\alpha + \\beta = \\gamma}} $$\n$$ \\bbox[border: 2px solid green; background: yellow]{\\color{red}{\\alpha + \\beta = \\gamma}} $$\n</code></pre> 3.\u6548\u679c\uff1a $$ \\color{red}{\\alpha + \\beta = \\gamma} $$ $$ \\bbox[yellow]{\\color{red}{\\alpha + \\beta = \\gamma}} $$ $$ \\bbox[border: 2px solid green; background: yellow]{\\color{red}{\\alpha + \\beta = \\gamma}} $$  \u6240\u6709\u516c\u5f0f\u7edf\u4e00\u989c\u8272 <p>1.\u521b\u5efa\u5e76\u7f16\u8f91\u81ea\u5b9a\u4e49 CSS \u6587\u4ef6\uff1a\u5728 docs/_css/extra.css \u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9\u6765\u66f4\u6539 LaTeX \u516c\u5f0f\u7684\u989c\u8272\u3002 </p>CSS<pre><code>/* \u66f4\u6539\u884c\u5185\u516c\u5f0f\u7684\u989c\u8272 */\n.md-typeset .math.inline {\n    color: red;\n    &lt;!-- background-color: lightgrey --&gt;\n    &lt;!-- \u6682\u672a\u5c1d\u8bd5 --&gt;\n}\n/* \u66f4\u6539\u5757\u7ea7\u516c\u5f0f\u7684\u989c\u8272 */\n.md-typeset .math.display {\n    color: blue;\n    &lt;!-- background-color: lightgrey --&gt;\n}\n</code></pre> 2.\u66f4\u65b0 mkdocs.yml \u914d\u7f6e\u6587\u4ef6\uff1a\u5c06 extra.css \u6587\u4ef6\u6dfb\u52a0\u5230 extra_css \u5217\u8868\u4e2d\uff1a YAML<pre><code>extra_css:\n- _css/extra.css  # \u786e\u4fdd\u8def\u5f84\u6b63\u786e\n</code></pre> 3.\u6784\u5efa\u548c\u9884\u89c8\uff1a\u4fdd\u5b58\u66f4\u6539\u540e\uff0c\u91cd\u65b0\u6784\u5efa\u4f60\u7684 MkDocs \u7ad9\u70b9\uff0c\u5e76\u9884\u89c8\u7ed3\u679c\u4ee5\u786e\u4fdd\u989c\u8272\u8bbe\u7f6e\u6b63\u786e PS:\u5982\u679c\u4f60\u6709\u7279\u5b9a\u7684\u989c\u8272\u9700\u6c42\uff0c\u53ef\u4ee5\u4f7f\u7528\u5176\u4ed6\u989c\u8272\u503c\uff0c\u5982 #ff0000\uff08\u7ea2\u8272\uff09\u3002"}]}